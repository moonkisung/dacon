{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a2fdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.7.1+cu110\n",
      "Cuda available: True\n",
      "Torch geometric version: 1.7.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pysmiles import read_smiles\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch_geometric\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import deepchem as dc\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "print(f\"Torch geometric version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d9c64",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8961c8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, root, filename, test=False, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        root = Where the dataset should be stored. This folder is split\n",
    "        into raw_dir (downloaded dataset) and processed_dir (processed data). \n",
    "        \"\"\"\n",
    "        self.test = test\n",
    "        self.filename = filename\n",
    "        super(MoleculeDataset, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\" If this file exists in raw_dir, the download is not triggered.\n",
    "            (The download func. is not implemented here)  \n",
    "        \"\"\"\n",
    "        return self.filename\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\" If these files are found in raw_dir, processing is skipped\"\"\"\n",
    "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
    "\n",
    "        if self.test:\n",
    "            return [f'data_test_{i}.pt' for i in list(self.data.index)]\n",
    "        else:\n",
    "            return [f'data_{i}.pt' for i in list(self.data.index)]\n",
    "        \n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
    "        #self.data = self.data.iloc[29628:]\n",
    "        #print(self.data.iloc[-1])\n",
    "        featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
    "        for index, mol in tqdm(self.data.iterrows(), total=self.data.shape[0]):\n",
    "            f = featurizer.featurize(mol[\"SMILES\"])\n",
    "            data = f[0].to_pyg_graph()\n",
    "            data.smiles = mol[\"SMILES\"]\n",
    "            if not self.test:\n",
    "                data.y = self._get_label(mol[\"target\"])\n",
    "            if self.test:\n",
    "                torch.save(data, \n",
    "                    os.path.join(self.processed_dir, \n",
    "                                 f'data_test_{index}.pt'))\n",
    "            else:\n",
    "                torch.save(data, \n",
    "                    os.path.join(self.processed_dir, \n",
    "                                 f'data_{index}.pt'))\n",
    "            \n",
    "\n",
    "    def _get_label(self, label):\n",
    "        if not self.test:\n",
    "            label = np.asarray([label])\n",
    "            return torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "    def len(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\" - Equivalent to __getitem__ in pytorch\n",
    "            - Is not needed for PyG's InMemoryDataset\n",
    "        \"\"\"\n",
    "        if self.test:\n",
    "            data = torch.load(os.path.join(self.processed_dir, \n",
    "                                 f'data_test_{idx}.pt'))\n",
    "        else:\n",
    "            data = torch.load(os.path.join(self.processed_dir, \n",
    "                                 f'data_{idx}.pt'))        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b1acc",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7488ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 200\n",
    "num_layers = 1\n",
    "dropout_rate = 0.1\n",
    "embedding_size = 128\n",
    "learning_rate = 1e-4\n",
    "vision_pretrain = True\n",
    "save_path = \"./models/best_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89a452d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MoleculeDataset(root=\"../data/\", filename=\"new_train.csv\")\n",
    "train_dataset  = train_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f0bbbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MoleculeDataset(30344)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0de1428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24275"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(train_dataset)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c30d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset[:24275], batch_size=BATCH_SIZE, num_workers=0, shuffle=True)\n",
    "val_dataloader = DataLoader(train_dataset[24275:], batch_size=BATCH_SIZE, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0877563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(batch=[1592], edge_attr=[3412, 11], edge_index=[2, 3412], ptr=[65], smiles=[64], x=[1592, 30], y=[64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = next(iter(train_dataloader))\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a381c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F \n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "num_features = train_dataset.num_features\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # Init parent\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        # GCN layers\n",
    "        self.initial_conv = GCNConv(num_features, embedding_size)\n",
    "        self.conv1 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv2 = GCNConv(embedding_size, embedding_size)\n",
    "        self.conv3 = GCNConv(embedding_size, embedding_size)\n",
    "\n",
    "        # Output layer\n",
    "        self.out = Linear(embedding_size*2, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        # First Conv layer\n",
    "        hidden = self.initial_conv(x, edge_index)\n",
    "        hidden = torch.tanh(hidden)\n",
    "\n",
    "        # Other Conv layers\n",
    "        hidden = self.conv1(hidden, edge_index)\n",
    "        hidden = torch.tanh(hidden)\n",
    "        #hidden = F.dropout(hidden, p=0.5, training=self.training)\n",
    "        hidden = self.conv2(hidden, edge_index)\n",
    "        hidden = torch.tanh(hidden)\n",
    "        #hidden = F.dropout(hidden, p=0.5, training=self.training)\n",
    "        hidden = self.conv3(hidden, edge_index)\n",
    "        hidden = torch.tanh(hidden)\n",
    "        #hidden = F.dropout(hidden, p=0.5, training=self.training)\n",
    "        \n",
    "        # Global Pooling (stack different aggregations)\n",
    "        hidden = torch.cat([gmp(hidden, batch_index), \n",
    "                            gap(hidden, batch_index)], dim=1)\n",
    "\n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.out(hidden)\n",
    "\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0abed998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  53761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (initial_conv): GCNConv(30, 128)\n",
       "  (conv1): GCNConv(128, 128)\n",
       "  (conv2): GCNConv(128, 128)\n",
       "  (conv3): GCNConv(128, 128)\n",
       "  (out): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN()\n",
    "model = model.to(device)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6507ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d4587e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch_item, epoch, batch, training):\n",
    "    x = batch_item['x'].float()\n",
    "    edge_index = batch_item['edge_index']\n",
    "    batch = batch_item['batch']\n",
    "    label = batch_item['y']\n",
    "\n",
    "    if training is True:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output, h = model(x, edge_index, batch)\n",
    "            #print(output.shape)\n",
    "            #print(label.shape)\n",
    "            loss = criterion(output.view(-1), label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output, h = model(x, edge_index, batch)\n",
    "            #print(output.shape)\n",
    "            #print(label.shape)            \n",
    "            loss = criterion(output.view(-1), label)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4894631e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "380it [00:08, 43.20it/s, Epoch=1, Loss=0.309303, Total Loss=0.292115]\n",
      "95it [00:02, 46.56it/s, Epoch=1, Val Loss=0.245225, Total Val Loss=0.264908]\n",
      "380it [00:08, 43.18it/s, Epoch=2, Loss=0.240097, Total Loss=0.260145]\n",
      "95it [00:01, 49.31it/s, Epoch=2, Val Loss=0.277595, Total Val Loss=0.255820]\n",
      "380it [00:08, 42.67it/s, Epoch=3, Loss=0.258524, Total Loss=0.254361]\n",
      "95it [00:02, 47.16it/s, Epoch=3, Val Loss=0.226521, Total Val Loss=0.250840]\n",
      "380it [00:08, 43.63it/s, Epoch=4, Loss=0.359437, Total Loss=0.250163]\n",
      "95it [00:02, 45.82it/s, Epoch=4, Val Loss=0.251308, Total Val Loss=0.247929]\n",
      "380it [00:08, 42.81it/s, Epoch=5, Loss=0.225301, Total Loss=0.246315]\n",
      "95it [00:02, 47.03it/s, Epoch=5, Val Loss=0.217967, Total Val Loss=0.243746]\n",
      "380it [00:08, 45.60it/s, Epoch=6, Loss=0.261388, Total Loss=0.243561]\n",
      "95it [00:02, 46.33it/s, Epoch=6, Val Loss=0.253291, Total Val Loss=0.241253]\n",
      "380it [00:08, 45.70it/s, Epoch=7, Loss=0.186062, Total Loss=0.240277]\n",
      "95it [00:01, 55.78it/s, Epoch=7, Val Loss=0.207692, Total Val Loss=0.238891]\n",
      "380it [00:08, 44.45it/s, Epoch=8, Loss=0.257401, Total Loss=0.237058]\n",
      "95it [00:02, 46.91it/s, Epoch=8, Val Loss=0.285649, Total Val Loss=0.235212]\n",
      "380it [00:09, 38.67it/s, Epoch=9, Loss=0.144225, Total Loss=0.235046]\n",
      "95it [00:02, 42.03it/s, Epoch=9, Val Loss=0.259145, Total Val Loss=0.236810]\n",
      "380it [00:08, 42.66it/s, Epoch=10, Loss=0.203608, Total Loss=0.232703]\n",
      "95it [00:02, 47.44it/s, Epoch=10, Val Loss=0.242874, Total Val Loss=0.231146]\n",
      "380it [00:08, 46.42it/s, Epoch=11, Loss=0.166388, Total Loss=0.231029]\n",
      "95it [00:01, 49.78it/s, Epoch=11, Val Loss=0.214141, Total Val Loss=0.242614]\n",
      "380it [00:09, 39.66it/s, Epoch=12, Loss=0.172243, Total Loss=0.229487]\n",
      "95it [00:01, 58.53it/s, Epoch=12, Val Loss=0.245514, Total Val Loss=0.228777]\n",
      "380it [00:08, 46.02it/s, Epoch=13, Loss=0.249822, Total Loss=0.228196]\n",
      "95it [00:01, 49.68it/s, Epoch=13, Val Loss=0.276574, Total Val Loss=0.226716]\n",
      "380it [00:08, 45.48it/s, Epoch=14, Loss=0.206491, Total Loss=0.226169]\n",
      "95it [00:01, 59.71it/s, Epoch=14, Val Loss=0.192771, Total Val Loss=0.232217]\n",
      "380it [00:08, 45.80it/s, Epoch=15, Loss=0.278199, Total Loss=0.225791]\n",
      "95it [00:01, 53.14it/s, Epoch=15, Val Loss=0.251160, Total Val Loss=0.224273]\n",
      "380it [00:08, 45.17it/s, Epoch=16, Loss=0.269948, Total Loss=0.223780]\n",
      "95it [00:01, 56.43it/s, Epoch=16, Val Loss=0.196078, Total Val Loss=0.223465]\n",
      "380it [00:09, 39.66it/s, Epoch=17, Loss=0.225522, Total Loss=0.223062]\n",
      "95it [00:01, 48.05it/s, Epoch=17, Val Loss=0.224418, Total Val Loss=0.223026]\n",
      "380it [00:08, 45.46it/s, Epoch=18, Loss=0.198595, Total Loss=0.221917]\n",
      "95it [00:01, 50.81it/s, Epoch=18, Val Loss=0.215205, Total Val Loss=0.225813]\n",
      "380it [00:08, 44.67it/s, Epoch=19, Loss=0.225279, Total Loss=0.221462]\n",
      "95it [00:01, 49.54it/s, Epoch=19, Val Loss=0.214283, Total Val Loss=0.220826]\n",
      "380it [00:08, 46.00it/s, Epoch=20, Loss=0.224627, Total Loss=0.220142]\n",
      "95it [00:01, 54.70it/s, Epoch=20, Val Loss=0.233692, Total Val Loss=0.220511]\n",
      "380it [00:08, 44.93it/s, Epoch=21, Loss=0.175905, Total Loss=0.219348]\n",
      "95it [00:01, 49.77it/s, Epoch=21, Val Loss=0.199258, Total Val Loss=0.219100]\n",
      "380it [00:08, 46.27it/s, Epoch=22, Loss=0.284585, Total Loss=0.219003]\n",
      "95it [00:01, 53.90it/s, Epoch=22, Val Loss=0.210294, Total Val Loss=0.218464]\n",
      "380it [00:08, 46.22it/s, Epoch=23, Loss=0.252414, Total Loss=0.217653]\n",
      "95it [00:01, 55.15it/s, Epoch=23, Val Loss=0.190482, Total Val Loss=0.218305]\n",
      "380it [00:08, 44.03it/s, Epoch=24, Loss=0.180326, Total Loss=0.217078]\n",
      "95it [00:01, 56.26it/s, Epoch=24, Val Loss=0.159327, Total Val Loss=0.218294]\n",
      "380it [00:08, 45.86it/s, Epoch=25, Loss=0.280938, Total Loss=0.216412]\n",
      "95it [00:01, 57.35it/s, Epoch=25, Val Loss=0.210529, Total Val Loss=0.217790]\n",
      "380it [00:08, 46.39it/s, Epoch=26, Loss=0.195014, Total Loss=0.215933]\n",
      "95it [00:01, 53.95it/s, Epoch=26, Val Loss=0.226791, Total Val Loss=0.217047]\n",
      "380it [00:08, 45.39it/s, Epoch=27, Loss=0.215821, Total Loss=0.214847]\n",
      "95it [00:01, 48.22it/s, Epoch=27, Val Loss=0.177183, Total Val Loss=0.216007]\n",
      "380it [00:08, 45.20it/s, Epoch=28, Loss=0.301088, Total Loss=0.214485]\n",
      "95it [00:01, 55.42it/s, Epoch=28, Val Loss=0.250779, Total Val Loss=0.215009]\n",
      "380it [00:09, 41.41it/s, Epoch=29, Loss=0.210341, Total Loss=0.213707]\n",
      "95it [00:01, 60.05it/s, Epoch=29, Val Loss=0.233109, Total Val Loss=0.215443]\n",
      "380it [00:08, 45.37it/s, Epoch=30, Loss=0.168829, Total Loss=0.212448]\n",
      "95it [00:01, 55.67it/s, Epoch=30, Val Loss=0.205319, Total Val Loss=0.215227]\n",
      "380it [00:08, 46.59it/s, Epoch=31, Loss=0.223097, Total Loss=0.212537]\n",
      "95it [00:01, 47.64it/s, Epoch=31, Val Loss=0.168218, Total Val Loss=0.215797]\n",
      "380it [00:09, 41.26it/s, Epoch=32, Loss=0.195876, Total Loss=0.211986]\n",
      "95it [00:01, 51.81it/s, Epoch=32, Val Loss=0.203656, Total Val Loss=0.213209]\n",
      "380it [00:08, 44.86it/s, Epoch=33, Loss=0.249382, Total Loss=0.211384]\n",
      "95it [00:01, 53.65it/s, Epoch=33, Val Loss=0.227463, Total Val Loss=0.212820]\n",
      "380it [00:08, 45.62it/s, Epoch=34, Loss=0.135570, Total Loss=0.211826]\n",
      "95it [00:01, 57.09it/s, Epoch=34, Val Loss=0.225991, Total Val Loss=0.213459]\n",
      "380it [00:08, 46.52it/s, Epoch=35, Loss=0.175027, Total Loss=0.209799]\n",
      "95it [00:01, 54.50it/s, Epoch=35, Val Loss=0.205354, Total Val Loss=0.212517]\n",
      "380it [00:09, 42.06it/s, Epoch=36, Loss=0.257489, Total Loss=0.209762]\n",
      "95it [00:01, 50.83it/s, Epoch=36, Val Loss=0.198621, Total Val Loss=0.212121]\n",
      "380it [00:08, 45.90it/s, Epoch=37, Loss=0.177159, Total Loss=0.209147]\n",
      "95it [00:01, 58.33it/s, Epoch=37, Val Loss=0.193799, Total Val Loss=0.212107]\n",
      "380it [00:09, 40.83it/s, Epoch=38, Loss=0.218765, Total Loss=0.208960]\n",
      "95it [00:01, 49.58it/s, Epoch=38, Val Loss=0.223406, Total Val Loss=0.214388]\n",
      "380it [00:08, 44.29it/s, Epoch=39, Loss=0.209039, Total Loss=0.208543]\n",
      "95it [00:01, 49.12it/s, Epoch=39, Val Loss=0.238584, Total Val Loss=0.213094]\n",
      "380it [00:08, 45.48it/s, Epoch=40, Loss=0.240173, Total Loss=0.207938]\n",
      "95it [00:02, 45.74it/s, Epoch=40, Val Loss=0.202089, Total Val Loss=0.211571]\n",
      "380it [00:08, 45.12it/s, Epoch=41, Loss=0.213878, Total Loss=0.208197]\n",
      "95it [00:01, 51.23it/s, Epoch=41, Val Loss=0.250624, Total Val Loss=0.210796]\n",
      "380it [00:08, 46.31it/s, Epoch=42, Loss=0.218242, Total Loss=0.206930]\n",
      "95it [00:01, 57.47it/s, Epoch=42, Val Loss=0.179597, Total Val Loss=0.209300]\n",
      "380it [00:09, 40.29it/s, Epoch=43, Loss=0.157669, Total Loss=0.206553]\n",
      "95it [00:01, 49.74it/s, Epoch=43, Val Loss=0.212830, Total Val Loss=0.212693]\n",
      "380it [00:08, 44.94it/s, Epoch=44, Loss=0.239676, Total Loss=0.206009]\n",
      "95it [00:01, 47.72it/s, Epoch=44, Val Loss=0.215370, Total Val Loss=0.209388]\n",
      "380it [00:08, 45.48it/s, Epoch=45, Loss=0.181928, Total Loss=0.205682]\n",
      "95it [00:02, 45.74it/s, Epoch=45, Val Loss=0.182804, Total Val Loss=0.209379]\n",
      "380it [00:09, 38.85it/s, Epoch=46, Loss=0.140150, Total Loss=0.204658]\n",
      "95it [00:01, 53.34it/s, Epoch=46, Val Loss=0.236983, Total Val Loss=0.213221]\n",
      "380it [00:08, 46.17it/s, Epoch=47, Loss=0.186973, Total Loss=0.204890]\n",
      "95it [00:01, 48.50it/s, Epoch=47, Val Loss=0.266785, Total Val Loss=0.210122]\n",
      "380it [00:08, 44.64it/s, Epoch=48, Loss=0.198727, Total Loss=0.204222]\n",
      "95it [00:01, 49.79it/s, Epoch=48, Val Loss=0.223883, Total Val Loss=0.207205]\n",
      "380it [00:08, 44.08it/s, Epoch=49, Loss=0.234442, Total Loss=0.203785]\n",
      "95it [00:01, 54.15it/s, Epoch=49, Val Loss=0.215944, Total Val Loss=0.207114]\n",
      "380it [00:08, 44.04it/s, Epoch=50, Loss=0.116840, Total Loss=0.203087]\n",
      "95it [00:01, 48.93it/s, Epoch=50, Val Loss=0.240784, Total Val Loss=0.206301]\n",
      "380it [00:08, 43.86it/s, Epoch=51, Loss=0.175928, Total Loss=0.203748]\n",
      "95it [00:01, 47.85it/s, Epoch=51, Val Loss=0.212415, Total Val Loss=0.206921]\n",
      "380it [00:08, 44.29it/s, Epoch=52, Loss=0.205942, Total Loss=0.202562]\n",
      "95it [00:01, 51.76it/s, Epoch=52, Val Loss=0.196801, Total Val Loss=0.206058]\n",
      "380it [00:08, 44.87it/s, Epoch=53, Loss=0.203501, Total Loss=0.201733]\n",
      "95it [00:02, 44.38it/s, Epoch=53, Val Loss=0.219294, Total Val Loss=0.207151]\n",
      "380it [00:09, 38.42it/s, Epoch=54, Loss=0.132735, Total Loss=0.201919]\n",
      "95it [00:02, 46.49it/s, Epoch=54, Val Loss=0.200641, Total Val Loss=0.206035]\n",
      "380it [00:08, 45.27it/s, Epoch=55, Loss=0.177168, Total Loss=0.201368]\n",
      "95it [00:01, 48.53it/s, Epoch=55, Val Loss=0.199706, Total Val Loss=0.212743]\n",
      "380it [00:10, 34.91it/s, Epoch=56, Loss=0.208286, Total Loss=0.200848]\n",
      "95it [00:02, 46.47it/s, Epoch=56, Val Loss=0.241263, Total Val Loss=0.205635]\n",
      "380it [00:09, 40.17it/s, Epoch=57, Loss=0.197391, Total Loss=0.200695]\n",
      "95it [00:01, 54.51it/s, Epoch=57, Val Loss=0.224362, Total Val Loss=0.206049]\n",
      "380it [00:08, 43.91it/s, Epoch=58, Loss=0.237477, Total Loss=0.200259]\n",
      "95it [00:02, 44.53it/s, Epoch=58, Val Loss=0.167538, Total Val Loss=0.203735]\n",
      "380it [00:08, 42.61it/s, Epoch=59, Loss=0.241767, Total Loss=0.199850]\n",
      "95it [00:01, 48.61it/s, Epoch=59, Val Loss=0.167886, Total Val Loss=0.204092]\n",
      "380it [00:08, 46.28it/s, Epoch=60, Loss=0.171436, Total Loss=0.198969]\n",
      "95it [00:01, 48.23it/s, Epoch=60, Val Loss=0.203549, Total Val Loss=0.205907]\n",
      "380it [00:08, 42.49it/s, Epoch=61, Loss=0.197231, Total Loss=0.199108]\n",
      "95it [00:01, 48.70it/s, Epoch=61, Val Loss=0.170973, Total Val Loss=0.207986]\n",
      "380it [00:08, 43.94it/s, Epoch=62, Loss=0.198467, Total Loss=0.198477]\n",
      "95it [00:01, 53.01it/s, Epoch=62, Val Loss=0.172830, Total Val Loss=0.208044]\n",
      "380it [00:08, 43.27it/s, Epoch=63, Loss=0.292998, Total Loss=0.198148]\n",
      "95it [00:01, 52.92it/s, Epoch=63, Val Loss=0.247055, Total Val Loss=0.205402]\n",
      "380it [00:08, 44.97it/s, Epoch=64, Loss=0.279752, Total Loss=0.198481]\n",
      "95it [00:01, 50.97it/s, Epoch=64, Val Loss=0.249858, Total Val Loss=0.201937]\n",
      "380it [00:08, 46.33it/s, Epoch=65, Loss=0.163964, Total Loss=0.197268]\n",
      "95it [00:01, 50.72it/s, Epoch=65, Val Loss=0.210207, Total Val Loss=0.202991]\n",
      "380it [00:09, 40.44it/s, Epoch=66, Loss=0.210318, Total Loss=0.197932]\n",
      "95it [00:01, 55.95it/s, Epoch=66, Val Loss=0.209448, Total Val Loss=0.201901]\n",
      "380it [00:08, 45.65it/s, Epoch=67, Loss=0.199391, Total Loss=0.196769]\n",
      "95it [00:01, 48.93it/s, Epoch=67, Val Loss=0.182346, Total Val Loss=0.201608]\n",
      "380it [00:08, 45.77it/s, Epoch=68, Loss=0.236808, Total Loss=0.196003]\n",
      "95it [00:01, 49.01it/s, Epoch=68, Val Loss=0.186408, Total Val Loss=0.201284]\n",
      "380it [00:08, 44.37it/s, Epoch=69, Loss=0.274103, Total Loss=0.196300]\n",
      "95it [00:01, 49.52it/s, Epoch=69, Val Loss=0.187814, Total Val Loss=0.201128]\n",
      "380it [00:08, 42.58it/s, Epoch=70, Loss=0.176374, Total Loss=0.195326]\n",
      "95it [00:02, 44.52it/s, Epoch=70, Val Loss=0.166596, Total Val Loss=0.202288]\n",
      "380it [00:09, 42.17it/s, Epoch=71, Loss=0.198881, Total Loss=0.195375]\n",
      "95it [00:01, 51.40it/s, Epoch=71, Val Loss=0.202088, Total Val Loss=0.203629]\n",
      "380it [00:08, 43.88it/s, Epoch=72, Loss=0.219346, Total Loss=0.194518]\n",
      "95it [00:01, 49.61it/s, Epoch=72, Val Loss=0.207909, Total Val Loss=0.200766]\n",
      "380it [00:08, 43.76it/s, Epoch=73, Loss=0.185152, Total Loss=0.194544]\n",
      "95it [00:01, 49.29it/s, Epoch=73, Val Loss=0.209053, Total Val Loss=0.200569]\n",
      "380it [00:08, 46.59it/s, Epoch=74, Loss=0.125546, Total Loss=0.193691]\n",
      "95it [00:01, 53.22it/s, Epoch=74, Val Loss=0.233541, Total Val Loss=0.200207]\n",
      "380it [00:08, 44.84it/s, Epoch=75, Loss=0.208515, Total Loss=0.195045]\n",
      "95it [00:02, 42.05it/s, Epoch=75, Val Loss=0.188607, Total Val Loss=0.199812]\n",
      "380it [00:08, 43.49it/s, Epoch=76, Loss=0.157010, Total Loss=0.192978]\n",
      "95it [00:01, 50.63it/s, Epoch=76, Val Loss=0.210174, Total Val Loss=0.199717]\n",
      "380it [00:08, 42.97it/s, Epoch=77, Loss=0.235521, Total Loss=0.193481]\n",
      "95it [00:01, 52.85it/s, Epoch=77, Val Loss=0.214428, Total Val Loss=0.204795]\n",
      "380it [00:09, 40.48it/s, Epoch=78, Loss=0.105752, Total Loss=0.192456]\n",
      "95it [00:01, 50.49it/s, Epoch=78, Val Loss=0.187774, Total Val Loss=0.199105]\n",
      "380it [00:08, 45.34it/s, Epoch=79, Loss=0.162689, Total Loss=0.193083]\n",
      "95it [00:01, 59.68it/s, Epoch=79, Val Loss=0.214668, Total Val Loss=0.202633]\n",
      "380it [00:08, 46.25it/s, Epoch=80, Loss=0.163045, Total Loss=0.191932]\n",
      "95it [00:01, 55.04it/s, Epoch=80, Val Loss=0.196106, Total Val Loss=0.200699]\n",
      "380it [00:09, 39.41it/s, Epoch=81, Loss=0.174221, Total Loss=0.192006]\n",
      "95it [00:01, 59.62it/s, Epoch=81, Val Loss=0.221512, Total Val Loss=0.208555]\n",
      "380it [00:08, 46.15it/s, Epoch=82, Loss=0.157948, Total Loss=0.192294]\n",
      "95it [00:01, 56.86it/s, Epoch=82, Val Loss=0.249353, Total Val Loss=0.205006]\n",
      "380it [00:08, 45.23it/s, Epoch=83, Loss=0.196426, Total Loss=0.191576]\n",
      "95it [00:01, 59.47it/s, Epoch=83, Val Loss=0.252164, Total Val Loss=0.197458]\n",
      "380it [00:08, 46.38it/s, Epoch=84, Loss=0.192513, Total Loss=0.190853]\n",
      "95it [00:01, 59.76it/s, Epoch=84, Val Loss=0.133415, Total Val Loss=0.200598]\n",
      "380it [00:08, 45.90it/s, Epoch=85, Loss=0.203938, Total Loss=0.190704]\n",
      "95it [00:02, 46.74it/s, Epoch=85, Val Loss=0.187017, Total Val Loss=0.199272]\n",
      "380it [00:08, 43.47it/s, Epoch=86, Loss=0.169642, Total Loss=0.190278]\n",
      "95it [00:01, 56.07it/s, Epoch=86, Val Loss=0.163676, Total Val Loss=0.199036]\n",
      "380it [00:08, 45.43it/s, Epoch=87, Loss=0.293731, Total Loss=0.190219]\n",
      "95it [00:01, 53.48it/s, Epoch=87, Val Loss=0.197018, Total Val Loss=0.197424]\n",
      "380it [00:08, 43.55it/s, Epoch=88, Loss=0.187716, Total Loss=0.189519]\n",
      "95it [00:01, 52.19it/s, Epoch=88, Val Loss=0.224957, Total Val Loss=0.197531]\n",
      "380it [00:08, 44.18it/s, Epoch=89, Loss=0.125647, Total Loss=0.189230]\n",
      "95it [00:01, 58.25it/s, Epoch=89, Val Loss=0.188383, Total Val Loss=0.196488]\n",
      "380it [00:08, 46.29it/s, Epoch=90, Loss=0.153507, Total Loss=0.189412]\n",
      "95it [00:01, 52.41it/s, Epoch=90, Val Loss=0.200508, Total Val Loss=0.197269]\n",
      "380it [00:08, 45.21it/s, Epoch=91, Loss=0.185329, Total Loss=0.189013]\n",
      "95it [00:01, 53.69it/s, Epoch=91, Val Loss=0.165840, Total Val Loss=0.197086]\n",
      "380it [00:08, 45.50it/s, Epoch=92, Loss=0.192982, Total Loss=0.189047]\n",
      "95it [00:01, 54.86it/s, Epoch=92, Val Loss=0.151921, Total Val Loss=0.195990]\n",
      "380it [00:08, 44.29it/s, Epoch=93, Loss=0.163012, Total Loss=0.188707]\n",
      "95it [00:01, 48.21it/s, Epoch=93, Val Loss=0.185238, Total Val Loss=0.195719]\n",
      "380it [00:08, 45.18it/s, Epoch=94, Loss=0.160239, Total Loss=0.188116]\n",
      "95it [00:01, 47.87it/s, Epoch=94, Val Loss=0.218625, Total Val Loss=0.195546]\n",
      "380it [00:08, 44.09it/s, Epoch=95, Loss=0.194773, Total Loss=0.187615]\n",
      "95it [00:01, 48.49it/s, Epoch=95, Val Loss=0.252218, Total Val Loss=0.197501]\n",
      "380it [00:08, 42.58it/s, Epoch=96, Loss=0.204128, Total Loss=0.187901]\n",
      "95it [00:01, 50.42it/s, Epoch=96, Val Loss=0.211753, Total Val Loss=0.197792]\n",
      "380it [00:08, 45.71it/s, Epoch=97, Loss=0.145562, Total Loss=0.187787]\n",
      "95it [00:02, 45.03it/s, Epoch=97, Val Loss=0.221527, Total Val Loss=0.202704]\n",
      "380it [00:09, 38.00it/s, Epoch=98, Loss=0.186435, Total Loss=0.187428]\n",
      "95it [00:02, 42.55it/s, Epoch=98, Val Loss=0.219416, Total Val Loss=0.201155]\n",
      "380it [00:08, 42.31it/s, Epoch=99, Loss=0.176041, Total Loss=0.187614]\n",
      "95it [00:01, 53.21it/s, Epoch=99, Val Loss=0.229004, Total Val Loss=0.194523]\n",
      "380it [00:08, 45.30it/s, Epoch=100, Loss=0.191607, Total Loss=0.187100]\n",
      "95it [00:02, 41.57it/s, Epoch=100, Val Loss=0.195335, Total Val Loss=0.194593]\n",
      "380it [00:08, 44.43it/s, Epoch=101, Loss=0.229736, Total Loss=0.186529]\n",
      "95it [00:01, 59.31it/s, Epoch=101, Val Loss=0.211951, Total Val Loss=0.196894]\n",
      "380it [00:08, 46.26it/s, Epoch=102, Loss=0.157033, Total Loss=0.185983]\n",
      "95it [00:01, 52.59it/s, Epoch=102, Val Loss=0.201394, Total Val Loss=0.195582]\n",
      "380it [00:08, 43.33it/s, Epoch=103, Loss=0.177436, Total Loss=0.186362]\n",
      "95it [00:01, 55.84it/s, Epoch=103, Val Loss=0.197735, Total Val Loss=0.194907]\n",
      "380it [00:08, 43.38it/s, Epoch=104, Loss=0.149400, Total Loss=0.185540]\n",
      "95it [00:01, 57.07it/s, Epoch=104, Val Loss=0.150182, Total Val Loss=0.194265]\n",
      "380it [00:08, 45.15it/s, Epoch=105, Loss=0.140900, Total Loss=0.185304]\n",
      "95it [00:01, 54.53it/s, Epoch=105, Val Loss=0.140357, Total Val Loss=0.194431]\n",
      "380it [00:08, 45.12it/s, Epoch=106, Loss=0.199750, Total Loss=0.185154]\n",
      "95it [00:01, 57.16it/s, Epoch=106, Val Loss=0.162918, Total Val Loss=0.195685]\n",
      "380it [00:08, 45.63it/s, Epoch=107, Loss=0.210021, Total Loss=0.184479]\n",
      "95it [00:01, 59.78it/s, Epoch=107, Val Loss=0.196800, Total Val Loss=0.193184]\n",
      "380it [00:08, 45.59it/s, Epoch=108, Loss=0.193915, Total Loss=0.184397]\n",
      "95it [00:01, 55.74it/s, Epoch=108, Val Loss=0.184246, Total Val Loss=0.194093]\n",
      "380it [00:08, 46.25it/s, Epoch=109, Loss=0.178940, Total Loss=0.183937]\n",
      "95it [00:01, 59.48it/s, Epoch=109, Val Loss=0.212624, Total Val Loss=0.194672]\n",
      "380it [00:08, 44.47it/s, Epoch=110, Loss=0.194474, Total Loss=0.184036]\n",
      "95it [00:01, 49.22it/s, Epoch=110, Val Loss=0.203612, Total Val Loss=0.193154]\n",
      "380it [00:08, 45.54it/s, Epoch=111, Loss=0.186353, Total Loss=0.184399]\n",
      "95it [00:01, 56.91it/s, Epoch=111, Val Loss=0.186705, Total Val Loss=0.196174]\n",
      "380it [00:08, 45.70it/s, Epoch=112, Loss=0.170370, Total Loss=0.183630]\n",
      "95it [00:01, 57.99it/s, Epoch=112, Val Loss=0.241527, Total Val Loss=0.197328]\n",
      "380it [00:08, 45.33it/s, Epoch=113, Loss=0.162673, Total Loss=0.183644]\n",
      "95it [00:01, 53.16it/s, Epoch=113, Val Loss=0.166941, Total Val Loss=0.193687]\n",
      "380it [00:08, 45.04it/s, Epoch=114, Loss=0.197966, Total Loss=0.183566]\n",
      "95it [00:01, 51.24it/s, Epoch=114, Val Loss=0.191535, Total Val Loss=0.193693]\n",
      "380it [00:08, 45.64it/s, Epoch=115, Loss=0.203090, Total Loss=0.183175]\n",
      "95it [00:01, 48.76it/s, Epoch=115, Val Loss=0.219692, Total Val Loss=0.192428]\n",
      "380it [00:08, 45.88it/s, Epoch=116, Loss=0.253969, Total Loss=0.182622]\n",
      "95it [00:01, 49.62it/s, Epoch=116, Val Loss=0.181131, Total Val Loss=0.194130]\n",
      "380it [00:08, 42.99it/s, Epoch=117, Loss=0.179029, Total Loss=0.182670]\n",
      "95it [00:01, 48.04it/s, Epoch=117, Val Loss=0.160489, Total Val Loss=0.192381]\n",
      "380it [00:09, 41.07it/s, Epoch=118, Loss=0.291374, Total Loss=0.182412]\n",
      "95it [00:01, 57.37it/s, Epoch=118, Val Loss=0.174222, Total Val Loss=0.193375]\n",
      "380it [00:08, 44.65it/s, Epoch=119, Loss=0.153327, Total Loss=0.182066]\n",
      "95it [00:02, 43.61it/s, Epoch=119, Val Loss=0.223175, Total Val Loss=0.192354]\n",
      "380it [00:09, 40.50it/s, Epoch=120, Loss=0.181144, Total Loss=0.182050]\n",
      "95it [00:02, 47.47it/s, Epoch=120, Val Loss=0.206812, Total Val Loss=0.191986]\n",
      "380it [00:09, 41.41it/s, Epoch=121, Loss=0.124511, Total Loss=0.182445]\n",
      "95it [00:01, 55.36it/s, Epoch=121, Val Loss=0.188450, Total Val Loss=0.192708]\n",
      "380it [00:08, 44.26it/s, Epoch=122, Loss=0.213895, Total Loss=0.181272]\n",
      "95it [00:01, 53.20it/s, Epoch=122, Val Loss=0.196759, Total Val Loss=0.194027]\n",
      "380it [00:09, 40.33it/s, Epoch=123, Loss=0.123543, Total Loss=0.181741]\n",
      "95it [00:02, 43.76it/s, Epoch=123, Val Loss=0.215935, Total Val Loss=0.191716]\n",
      "380it [00:08, 43.77it/s, Epoch=124, Loss=0.189431, Total Loss=0.181201]\n",
      "95it [00:01, 47.66it/s, Epoch=124, Val Loss=0.208234, Total Val Loss=0.191542]\n",
      "380it [00:08, 42.79it/s, Epoch=125, Loss=0.223926, Total Loss=0.180528]\n",
      "95it [00:01, 51.45it/s, Epoch=125, Val Loss=0.190056, Total Val Loss=0.191304]\n",
      "380it [00:08, 42.24it/s, Epoch=126, Loss=0.241837, Total Loss=0.180647]\n",
      "95it [00:01, 51.64it/s, Epoch=126, Val Loss=0.179388, Total Val Loss=0.190715]\n",
      "380it [00:08, 45.84it/s, Epoch=127, Loss=0.209956, Total Loss=0.180401]\n",
      "95it [00:02, 44.06it/s, Epoch=127, Val Loss=0.193838, Total Val Loss=0.193073]\n",
      "380it [00:08, 42.55it/s, Epoch=128, Loss=0.213887, Total Loss=0.180276]\n",
      "95it [00:01, 48.83it/s, Epoch=128, Val Loss=0.243703, Total Val Loss=0.192368]\n",
      "380it [00:08, 45.05it/s, Epoch=129, Loss=0.140258, Total Loss=0.180222]\n",
      "95it [00:02, 42.55it/s, Epoch=129, Val Loss=0.255620, Total Val Loss=0.190829]\n",
      "380it [00:08, 45.07it/s, Epoch=130, Loss=0.181223, Total Loss=0.179756]\n",
      "95it [00:02, 41.06it/s, Epoch=130, Val Loss=0.178662, Total Val Loss=0.189751]\n",
      "380it [00:08, 46.31it/s, Epoch=131, Loss=0.157291, Total Loss=0.179463]\n",
      "95it [00:01, 52.03it/s, Epoch=131, Val Loss=0.137142, Total Val Loss=0.189604]\n",
      "380it [00:08, 43.50it/s, Epoch=132, Loss=0.185655, Total Loss=0.180209]\n",
      "95it [00:01, 51.25it/s, Epoch=132, Val Loss=0.206106, Total Val Loss=0.192651]\n",
      "380it [00:08, 43.53it/s, Epoch=133, Loss=0.165031, Total Loss=0.179947]\n",
      "95it [00:02, 46.94it/s, Epoch=133, Val Loss=0.174891, Total Val Loss=0.191295]\n",
      "380it [00:08, 42.43it/s, Epoch=134, Loss=0.140504, Total Loss=0.178750]\n",
      "95it [00:01, 55.28it/s, Epoch=134, Val Loss=0.211987, Total Val Loss=0.189388]\n",
      "380it [00:08, 42.78it/s, Epoch=135, Loss=0.211718, Total Loss=0.179351]\n",
      "95it [00:01, 47.92it/s, Epoch=135, Val Loss=0.213204, Total Val Loss=0.190245]\n",
      "380it [00:08, 44.98it/s, Epoch=136, Loss=0.104697, Total Loss=0.178062]\n",
      "95it [00:01, 52.10it/s, Epoch=136, Val Loss=0.177404, Total Val Loss=0.189303]\n",
      "380it [00:08, 43.58it/s, Epoch=137, Loss=0.216912, Total Loss=0.178534]\n",
      "95it [00:02, 47.14it/s, Epoch=137, Val Loss=0.174139, Total Val Loss=0.190086]\n",
      "380it [00:09, 38.57it/s, Epoch=138, Loss=0.129608, Total Loss=0.178370]\n",
      "95it [00:01, 47.79it/s, Epoch=138, Val Loss=0.177331, Total Val Loss=0.194547]\n",
      "380it [00:08, 46.31it/s, Epoch=139, Loss=0.171833, Total Loss=0.178306]\n",
      "95it [00:01, 50.59it/s, Epoch=139, Val Loss=0.195262, Total Val Loss=0.189332]\n",
      "380it [00:08, 44.27it/s, Epoch=140, Loss=0.124618, Total Loss=0.177584]\n",
      "95it [00:01, 51.35it/s, Epoch=140, Val Loss=0.184666, Total Val Loss=0.190454]\n",
      "380it [00:08, 45.85it/s, Epoch=141, Loss=0.171156, Total Loss=0.178014]\n",
      "95it [00:02, 46.94it/s, Epoch=141, Val Loss=0.162035, Total Val Loss=0.189220]\n",
      "380it [00:08, 45.37it/s, Epoch=142, Loss=0.232903, Total Loss=0.177279]\n",
      "95it [00:01, 48.24it/s, Epoch=142, Val Loss=0.150007, Total Val Loss=0.188465]\n",
      "380it [00:08, 43.60it/s, Epoch=143, Loss=0.165669, Total Loss=0.176841]\n",
      "95it [00:01, 48.24it/s, Epoch=143, Val Loss=0.169963, Total Val Loss=0.189400]\n",
      "380it [00:08, 42.90it/s, Epoch=144, Loss=0.171710, Total Loss=0.177165]\n",
      "95it [00:01, 55.49it/s, Epoch=144, Val Loss=0.195189, Total Val Loss=0.192880]\n",
      "380it [00:09, 38.34it/s, Epoch=145, Loss=0.155037, Total Loss=0.176939]\n",
      "95it [00:01, 50.64it/s, Epoch=145, Val Loss=0.222205, Total Val Loss=0.189390]\n",
      "380it [00:09, 38.66it/s, Epoch=146, Loss=0.097004, Total Loss=0.176456]\n",
      "95it [00:01, 50.87it/s, Epoch=146, Val Loss=0.234522, Total Val Loss=0.194503]\n",
      "380it [00:09, 40.22it/s, Epoch=147, Loss=0.191895, Total Loss=0.176795]\n",
      "95it [00:02, 43.12it/s, Epoch=147, Val Loss=0.220400, Total Val Loss=0.189322]\n",
      "380it [00:11, 33.88it/s, Epoch=148, Loss=0.129426, Total Loss=0.176204]\n",
      "95it [00:01, 55.35it/s, Epoch=148, Val Loss=0.200874, Total Val Loss=0.189457]\n",
      "380it [00:10, 37.62it/s, Epoch=149, Loss=0.153836, Total Loss=0.176106]\n",
      "95it [00:01, 55.53it/s, Epoch=149, Val Loss=0.184691, Total Val Loss=0.187497]\n",
      "380it [00:08, 43.42it/s, Epoch=150, Loss=0.137284, Total Loss=0.175399]\n",
      "95it [00:02, 42.45it/s, Epoch=150, Val Loss=0.216711, Total Val Loss=0.189846]\n",
      "380it [00:08, 43.11it/s, Epoch=151, Loss=0.171387, Total Loss=0.176626]\n",
      "95it [00:01, 49.67it/s, Epoch=151, Val Loss=0.219153, Total Val Loss=0.195272]\n",
      "380it [00:09, 41.87it/s, Epoch=152, Loss=0.209453, Total Loss=0.175203]\n",
      "95it [00:02, 43.05it/s, Epoch=152, Val Loss=0.210500, Total Val Loss=0.188340]\n",
      "380it [00:08, 45.58it/s, Epoch=153, Loss=0.146209, Total Loss=0.175138]\n",
      "95it [00:01, 47.67it/s, Epoch=153, Val Loss=0.208449, Total Val Loss=0.188374]\n",
      "380it [00:08, 43.63it/s, Epoch=154, Loss=0.216462, Total Loss=0.175681]\n",
      "95it [00:01, 51.23it/s, Epoch=154, Val Loss=0.168822, Total Val Loss=0.186908]\n",
      "380it [00:08, 44.91it/s, Epoch=155, Loss=0.187703, Total Loss=0.175014]\n",
      "95it [00:01, 49.35it/s, Epoch=155, Val Loss=0.200240, Total Val Loss=0.188396]\n",
      "380it [00:08, 44.58it/s, Epoch=156, Loss=0.137447, Total Loss=0.174686]\n",
      "95it [00:01, 47.85it/s, Epoch=156, Val Loss=0.197881, Total Val Loss=0.186998]\n",
      "380it [00:08, 45.07it/s, Epoch=157, Loss=0.206149, Total Loss=0.174725]\n",
      "95it [00:02, 43.47it/s, Epoch=157, Val Loss=0.170024, Total Val Loss=0.186960]\n",
      "380it [00:08, 42.52it/s, Epoch=158, Loss=0.147536, Total Loss=0.173849]\n",
      "95it [00:01, 48.64it/s, Epoch=158, Val Loss=0.169766, Total Val Loss=0.189138]\n",
      "380it [00:10, 36.53it/s, Epoch=159, Loss=0.192383, Total Loss=0.174210]\n",
      "95it [00:01, 55.82it/s, Epoch=159, Val Loss=0.154946, Total Val Loss=0.187292]\n",
      "380it [00:09, 39.49it/s, Epoch=160, Loss=0.179987, Total Loss=0.175335]\n",
      "95it [00:01, 54.51it/s, Epoch=160, Val Loss=0.197779, Total Val Loss=0.188701]\n",
      "380it [00:08, 44.93it/s, Epoch=161, Loss=0.123538, Total Loss=0.174549]\n",
      "95it [00:01, 52.28it/s, Epoch=161, Val Loss=0.206371, Total Val Loss=0.187125]\n",
      "380it [00:11, 32.48it/s, Epoch=162, Loss=0.149211, Total Loss=0.173883]\n",
      "95it [00:02, 44.71it/s, Epoch=162, Val Loss=0.177022, Total Val Loss=0.186422]\n",
      "380it [00:12, 30.47it/s, Epoch=163, Loss=0.190720, Total Loss=0.173413]\n",
      "95it [00:01, 47.52it/s, Epoch=163, Val Loss=0.171705, Total Val Loss=0.187124]\n",
      "380it [00:09, 38.30it/s, Epoch=164, Loss=0.175545, Total Loss=0.173257]\n",
      "95it [00:02, 46.31it/s, Epoch=164, Val Loss=0.203040, Total Val Loss=0.187790]\n",
      "380it [00:10, 36.45it/s, Epoch=165, Loss=0.185154, Total Loss=0.173749]\n",
      "95it [00:01, 52.62it/s, Epoch=165, Val Loss=0.189598, Total Val Loss=0.187052]\n",
      "380it [00:11, 34.53it/s, Epoch=166, Loss=0.173404, Total Loss=0.172685]\n",
      "95it [00:01, 49.40it/s, Epoch=166, Val Loss=0.163392, Total Val Loss=0.187352]\n",
      "380it [00:10, 36.82it/s, Epoch=167, Loss=0.169961, Total Loss=0.172903]\n",
      "95it [00:02, 45.59it/s, Epoch=167, Val Loss=0.188453, Total Val Loss=0.186243]\n",
      "380it [00:13, 28.12it/s, Epoch=168, Loss=0.213332, Total Loss=0.172677]\n",
      "95it [00:02, 39.59it/s, Epoch=168, Val Loss=0.195619, Total Val Loss=0.185245]\n",
      "380it [00:12, 29.83it/s, Epoch=169, Loss=0.175748, Total Loss=0.173306]\n",
      "95it [00:02, 47.07it/s, Epoch=169, Val Loss=0.166018, Total Val Loss=0.187237]\n",
      "380it [00:11, 33.29it/s, Epoch=170, Loss=0.178004, Total Loss=0.172391]\n",
      "95it [00:01, 52.56it/s, Epoch=170, Val Loss=0.172996, Total Val Loss=0.185640]\n",
      "380it [00:11, 34.54it/s, Epoch=171, Loss=0.150417, Total Loss=0.171827]\n",
      "95it [00:02, 43.71it/s, Epoch=171, Val Loss=0.192765, Total Val Loss=0.186727]\n",
      "380it [00:13, 27.69it/s, Epoch=172, Loss=0.174103, Total Loss=0.172014]\n",
      "95it [00:02, 44.47it/s, Epoch=172, Val Loss=0.160441, Total Val Loss=0.185509]\n",
      "380it [00:11, 32.49it/s, Epoch=173, Loss=0.223880, Total Loss=0.171782]\n",
      "95it [00:02, 46.40it/s, Epoch=173, Val Loss=0.149517, Total Val Loss=0.185175]\n",
      "380it [00:11, 33.88it/s, Epoch=174, Loss=0.124306, Total Loss=0.171769]\n",
      "95it [00:02, 46.58it/s, Epoch=174, Val Loss=0.162103, Total Val Loss=0.185031]\n",
      "380it [00:13, 28.67it/s, Epoch=175, Loss=0.158324, Total Loss=0.171112]\n",
      "95it [00:02, 41.05it/s, Epoch=175, Val Loss=0.224177, Total Val Loss=0.186698]\n",
      "380it [00:12, 29.31it/s, Epoch=176, Loss=0.167483, Total Loss=0.171851]\n",
      "95it [00:02, 41.32it/s, Epoch=176, Val Loss=0.191413, Total Val Loss=0.186492]\n",
      "380it [00:11, 33.47it/s, Epoch=177, Loss=0.187668, Total Loss=0.171070]\n",
      "95it [00:02, 43.71it/s, Epoch=177, Val Loss=0.210585, Total Val Loss=0.186157]\n",
      "380it [00:11, 32.34it/s, Epoch=178, Loss=0.138239, Total Loss=0.171393]\n",
      "95it [00:02, 44.25it/s, Epoch=178, Val Loss=0.229288, Total Val Loss=0.191155]\n",
      "380it [00:10, 35.48it/s, Epoch=179, Loss=0.108646, Total Loss=0.171043]\n",
      "95it [00:02, 46.27it/s, Epoch=179, Val Loss=0.168909, Total Val Loss=0.184960]\n",
      "380it [00:11, 32.77it/s, Epoch=180, Loss=0.188253, Total Loss=0.170873]\n",
      "95it [00:02, 42.49it/s, Epoch=180, Val Loss=0.213744, Total Val Loss=0.187122]\n",
      "380it [00:12, 31.53it/s, Epoch=181, Loss=0.124208, Total Loss=0.170164]\n",
      "95it [00:02, 44.41it/s, Epoch=181, Val Loss=0.207134, Total Val Loss=0.187340]\n",
      "380it [00:12, 29.52it/s, Epoch=182, Loss=0.178165, Total Loss=0.171070]\n",
      "95it [00:02, 40.61it/s, Epoch=182, Val Loss=0.161861, Total Val Loss=0.184474]\n",
      "380it [00:13, 28.76it/s, Epoch=183, Loss=0.176627, Total Loss=0.170058]\n",
      "95it [00:02, 42.40it/s, Epoch=183, Val Loss=0.167415, Total Val Loss=0.184304]\n",
      "380it [00:11, 31.99it/s, Epoch=184, Loss=0.240188, Total Loss=0.170120]\n",
      "95it [00:01, 47.68it/s, Epoch=184, Val Loss=0.183631, Total Val Loss=0.187221]\n",
      "380it [00:13, 28.80it/s, Epoch=185, Loss=0.192134, Total Loss=0.170596]\n",
      "95it [00:01, 48.41it/s, Epoch=185, Val Loss=0.182121, Total Val Loss=0.186977]\n",
      "380it [00:10, 36.76it/s, Epoch=186, Loss=0.144826, Total Loss=0.169605]\n",
      "95it [00:02, 43.06it/s, Epoch=186, Val Loss=0.230851, Total Val Loss=0.183922]\n",
      "380it [00:12, 29.48it/s, Epoch=187, Loss=0.197422, Total Loss=0.169549]\n",
      "95it [00:02, 43.94it/s, Epoch=187, Val Loss=0.227616, Total Val Loss=0.186153]\n",
      "380it [00:11, 34.36it/s, Epoch=188, Loss=0.222292, Total Loss=0.170159]\n",
      "95it [00:02, 43.08it/s, Epoch=188, Val Loss=0.167280, Total Val Loss=0.183680]\n",
      "380it [00:11, 31.97it/s, Epoch=189, Loss=0.151244, Total Loss=0.169607]\n",
      "95it [00:02, 42.15it/s, Epoch=189, Val Loss=0.192772, Total Val Loss=0.184782]\n",
      "380it [00:13, 28.99it/s, Epoch=190, Loss=0.121928, Total Loss=0.169082]\n",
      "95it [00:02, 41.83it/s, Epoch=190, Val Loss=0.187022, Total Val Loss=0.183824]\n",
      "380it [00:12, 30.92it/s, Epoch=191, Loss=0.143994, Total Loss=0.168795]\n",
      "95it [00:01, 48.21it/s, Epoch=191, Val Loss=0.205388, Total Val Loss=0.185517]\n",
      "380it [00:11, 32.06it/s, Epoch=192, Loss=0.165463, Total Loss=0.168988]\n",
      "95it [00:02, 43.64it/s, Epoch=192, Val Loss=0.151142, Total Val Loss=0.186168]\n",
      "380it [00:12, 30.57it/s, Epoch=193, Loss=0.153575, Total Loss=0.168228]\n",
      "95it [00:02, 46.60it/s, Epoch=193, Val Loss=0.175758, Total Val Loss=0.184384]\n",
      "380it [00:12, 30.71it/s, Epoch=194, Loss=0.122035, Total Loss=0.168209]\n",
      "95it [00:02, 43.64it/s, Epoch=194, Val Loss=0.176439, Total Val Loss=0.183509]\n",
      "380it [00:11, 32.98it/s, Epoch=195, Loss=0.163595, Total Loss=0.168213]\n",
      "95it [00:02, 42.09it/s, Epoch=195, Val Loss=0.147066, Total Val Loss=0.185608]\n",
      "380it [00:11, 33.62it/s, Epoch=196, Loss=0.157961, Total Loss=0.168224]\n",
      "95it [00:01, 53.81it/s, Epoch=196, Val Loss=0.158278, Total Val Loss=0.183245]\n",
      "380it [00:08, 43.40it/s, Epoch=197, Loss=0.172953, Total Loss=0.167937]\n",
      "95it [00:01, 55.11it/s, Epoch=197, Val Loss=0.189782, Total Val Loss=0.185370]\n",
      "380it [00:09, 42.20it/s, Epoch=198, Loss=0.223510, Total Loss=0.168177]\n",
      "95it [00:01, 51.56it/s, Epoch=198, Val Loss=0.167409, Total Val Loss=0.188731]\n",
      "380it [00:08, 44.03it/s, Epoch=199, Loss=0.156922, Total Loss=0.168561]\n",
      "95it [00:02, 45.05it/s, Epoch=199, Val Loss=0.193932, Total Val Loss=0.186797]\n",
      "380it [00:08, 43.24it/s, Epoch=200, Loss=0.223352, Total Loss=0.167590]\n",
      "95it [00:01, 56.33it/s, Epoch=200, Val Loss=0.218650, Total Val Loss=0.183230]\n"
     ]
    }
   ],
   "source": [
    "loss_plot, val_loss_plot = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss, total_val_loss = 0, 0\n",
    "    \n",
    "    tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
    "    training = True\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_item.to(device)\n",
    "        batch_loss = train_step(batch_item, epoch, batch, training)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Loss': '{:06f}'.format(batch_loss.item()),\n",
    "            'Total Loss' : '{:06f}'.format(total_loss/(batch+1))\n",
    "        })\n",
    "    loss_plot.append(total_loss/(batch+1))\n",
    "    tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
    "    training = False\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_item.to(device)\n",
    "        batch_loss = train_step(batch_item, epoch, batch, training)\n",
    "        total_val_loss += batch_loss\n",
    "        \n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Val Loss': '{:06f}'.format(batch_loss.item()),\n",
    "            'Total Val Loss' : '{:06f}'.format(total_val_loss/(batch+1))\n",
    "        })\n",
    "    val_loss_plot.append(total_val_loss/(batch+1))\n",
    "    if np.min(val_loss_plot) == val_loss_plot[-1]:\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55b03f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot, label='train_loss')\n",
    "plt.plot(val_loss_plot, label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss(mae)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "834fbea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = GCN()\n",
    "loaded_model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2321d7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1799,  0.1215, -0.0757,  ..., -0.0434, -0.0972, -0.1635],\n",
      "        [ 0.1750, -0.0348,  0.2011,  ..., -0.1269,  0.0118, -0.2038],\n",
      "        [ 0.0776, -0.2186,  0.1511,  ..., -0.2740,  0.0862,  0.2218],\n",
      "        ...,\n",
      "        [-0.1008, -0.0654,  0.0891,  ..., -0.2494,  0.3802,  0.3667],\n",
      "        [-0.1447, -0.0086, -0.1410,  ..., -0.0903,  0.0287,  0.1302],\n",
      "        [-0.0816,  0.0359,  0.1552,  ...,  0.0931,  0.0511, -0.0073]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in loaded_model.parameters():\n",
    "    print(param)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba74f845",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv')\n",
    "submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5382172a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>SMILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>COc1ccc(S(=O)(=O)NC2CCN(C3CCCCC3)CC2)c(C)c1C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>CC(CCCC1CCC2C3=C(CC[C@]12C)[C@@]1(C)CC[C@H](C)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>C[C@@H]1C[C@@H]1c1ccc2c(c1)c(-c1ccc[nH]c1=O)c(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>CCCn1c(=O)c2ccccc2n2c(SCC(=O)NC(Cc3ccccc3)c3cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>CC(C)CN(C[C@@H](O)[C@H](Cc1ccccc1)NC(=O)OCc1cn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>test_597</td>\n",
       "      <td>N#Cc1c(-n2c3ccccc3c3ccccc32)c(-n2c3ccccc3c3ccc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>test_598</td>\n",
       "      <td>CC1(C)c2ccccc2N(c2ccc(-c3cc(-c4ccc(N5c6ccccc6C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>test_599</td>\n",
       "      <td>Cc1nc(-c2ccc(N3c4ccccc4C(C)(C)c4ccccc43)cc2)cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>test_600</td>\n",
       "      <td>c1ccc2c(c1)Oc1ccccc1N2c1ccc(-c2nc3ccccc3s2)cc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>test_601</td>\n",
       "      <td>c1ccc2c(c1)Oc1ccccc1N2c1ccc(-c2nc3cc4sc(-c5ccc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>602 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          uid                                             SMILES\n",
       "0      test_0       COc1ccc(S(=O)(=O)NC2CCN(C3CCCCC3)CC2)c(C)c1C\n",
       "1      test_1  CC(CCCC1CCC2C3=C(CC[C@]12C)[C@@]1(C)CC[C@H](C)...\n",
       "2      test_2  C[C@@H]1C[C@@H]1c1ccc2c(c1)c(-c1ccc[nH]c1=O)c(...\n",
       "3      test_3  CCCn1c(=O)c2ccccc2n2c(SCC(=O)NC(Cc3ccccc3)c3cc...\n",
       "4      test_4  CC(C)CN(C[C@@H](O)[C@H](Cc1ccccc1)NC(=O)OCc1cn...\n",
       "..        ...                                                ...\n",
       "597  test_597  N#Cc1c(-n2c3ccccc3c3ccccc32)c(-n2c3ccccc3c3ccc...\n",
       "598  test_598  CC1(C)c2ccccc2N(c2ccc(-c3cc(-c4ccc(N5c6ccccc6C...\n",
       "599  test_599  Cc1nc(-c2ccc(N3c4ccccc4C(C)(C)c4ccccc43)cc2)cc...\n",
       "600  test_600     c1ccc2c(c1)Oc1ccccc1N2c1ccc(-c2nc3ccccc3s2)cc1\n",
       "601  test_601  c1ccc2c(c1)Oc1ccccc1N2c1ccc(-c2nc3cc4sc(-c5ccc...\n",
       "\n",
       "[602 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1aba58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MoleculeDataset(root=\"../data/\", filename=\"test.csv\", test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83e1e2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[56, 11], edge_index=[2, 56], smiles=\"COc1ccc(S(=O)(=O)NC2CCN(C3CCCCC3)CC2)c(C)c1C\", x=[26, 30])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d819c3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch_geometric.data.dataloader.DataLoader at 0x7f80bc22cbe0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=0, shuffle=False)\n",
    "test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d679fad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(batch=[1668], edge_attr=[3574, 11], edge_index=[2, 3574], ptr=[65], smiles=[64], x=[1668, 30])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = next(iter(test_dataloader))\n",
    "test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0277caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(batch=[1668], edge_attr=[3574, 11], edge_index=[2, 3574], ptr=[65], smiles=[64], x=[1668, 30])\n"
     ]
    }
   ],
   "source": [
    "for batch_item in test_dataloader:\n",
    "    print(batch_item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c87b78f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset):\n",
    "    #loaded_model.to(device)\n",
    "    loaded_model.eval()\n",
    "    result = []\n",
    "    for batch_item in dataset:\n",
    "        batch_item.to(device)\n",
    "        x = batch_item['x'].float()\n",
    "        edge_index = batch_item['edge_index']\n",
    "        batch = batch_item['batch']\n",
    "        with torch.no_grad():\n",
    "            output, h = loaded_model(x, edge_index, batch)\n",
    "        output = output[:, 0]\n",
    "        output = output.cpu().numpy()\n",
    "        result.extend(output)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c55e628",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensor for argument #3 'mat2' is on CPU, but expected it to be on GPU (while checking arguments for addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-242f24e61eae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-e054c6968afa>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_item\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#output = output.numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-72f45c20cd83>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, batch_index)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# First Conv layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor for argument #3 'mat2' is on CPU, but expected it to be on GPU (while checking arguments for addmm)"
     ]
    }
   ],
   "source": [
    "pred = predict(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbb778",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['ST1_GAP(eV)'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c9d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60897111",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a452ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('dacon_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff689b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dacon_submit_api import dacon_submit_api \n",
    "\n",
    "result = dacon_submit_api.post_submission_file(\n",
    "    'dacon_baseline.csv', \n",
    "    '59375e1f3b2f6e39215d683eaee1a165fb30ef348d1da88be153f64c17dfe759', \n",
    "    '235789', \n",
    "    'melona', \n",
    "    'gnn_2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1501fd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f487ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57337cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eafd8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc4630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71451e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be98d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
