{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7586317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.7.1+cu110\n",
      "Cuda available: True\n",
      "Torch geometric version: 1.7.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pysmiles import read_smiles\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch_geometric\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import deepchem as dc\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "print(f\"Torch geometric version: {torch_geometric.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e939ec1",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f8ebbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, root, filename, test=False, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        root = Where the dataset should be stored. This folder is split\n",
    "        into raw_dir (downloaded dataset) and processed_dir (processed data). \n",
    "        \"\"\"\n",
    "        self.test = test\n",
    "        self.filename = filename\n",
    "        super(MoleculeDataset, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\" If this file exists in raw_dir, the download is not triggered.\n",
    "            (The download func. is not implemented here)  \n",
    "        \"\"\"\n",
    "        return self.filename\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\" If these files are found in raw_dir, processing is skipped\"\"\"\n",
    "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
    "\n",
    "        if self.test:\n",
    "            return [f'data_test_{i}.pt' for i in list(self.data.index)]\n",
    "        else:\n",
    "            return [f'data_{i}.pt' for i in list(self.data.index)]\n",
    "        \n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
    "        #self.data = self.data.iloc[29628:]\n",
    "        #print(self.data.iloc[-1])\n",
    "        featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
    "        for index, mol in tqdm(self.data.iterrows(), total=self.data.shape[0]):\n",
    "            f = featurizer.featurize(mol[\"SMILES\"])\n",
    "            data = f[0].to_pyg_graph()\n",
    "            data.smiles = mol[\"SMILES\"]\n",
    "            if not self.test:\n",
    "                data.y = self._get_label(mol[\"target\"])\n",
    "            if self.test:\n",
    "                torch.save(data, \n",
    "                    os.path.join(self.processed_dir, \n",
    "                                 f'data_test_{index}.pt'))\n",
    "            else:\n",
    "                torch.save(data, \n",
    "                    os.path.join(self.processed_dir, \n",
    "                                 f'data_{index}.pt'))\n",
    "            \n",
    "\n",
    "    def _get_label(self, label):\n",
    "        if not self.test:\n",
    "            label = np.asarray([label])\n",
    "            return torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "    def len(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        \"\"\" - Equivalent to __getitem__ in pytorch\n",
    "            - Is not needed for PyG's InMemoryDataset\n",
    "        \"\"\"\n",
    "        if self.test:\n",
    "            data = torch.load(os.path.join(self.processed_dir, \n",
    "                                 f'data_test_{idx}.pt'))\n",
    "        else:\n",
    "            data = torch.load(os.path.join(self.processed_dir, \n",
    "                                 f'data_{idx}.pt'))        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f724ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MoleculeDataset(root=\"../data/\", filename=\"new_train.csv\")\n",
    "train_dataset  = train_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a1f79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24275"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(train_dataset)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c86e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset[:24275], batch_size=BATCH_SIZE, num_workers=0, shuffle=True)\n",
    "val_dataloader = DataLoader(train_dataset[24275:], batch_size=BATCH_SIZE, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b92739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(batch=[1567], edge_attr=[3362, 11], edge_index=[2, 3362], ptr=[65], smiles=[64], x=[1567, 30], y=[64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = next(iter(train_dataloader))\n",
    "sample_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24a234",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c478b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 200\n",
    "#num_layers = 1\n",
    "#dropout_rate = 0.1\n",
    "embedding_size = 128\n",
    "learning_rate = 1e-4\n",
    "vision_pretrain = True\n",
    "save_path = \"./models/sage/best_model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d30e5a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1613c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv, global_max_pool\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, dim_features, dim_target, config):\n",
    "        super().__init__()\n",
    "\n",
    "        num_layers = config['num_layers']\n",
    "        dim_embedding = config['dim_embedding']\n",
    "        self.aggregation = config['aggregation']  # can be mean or max\n",
    "\n",
    "        if self.aggregation == 'max':\n",
    "            self.fc_max = nn.Linear(dim_embedding, dim_embedding)\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for i in range(num_layers):\n",
    "            dim_input = dim_features if i == 0 else dim_embedding\n",
    "\n",
    "            conv = SAGEConv(dim_input, dim_embedding)\n",
    "            # Overwrite aggregation method (default is set to mean\n",
    "            conv.aggr = self.aggregation\n",
    "\n",
    "            self.layers.append(conv)\n",
    "\n",
    "        # For graph classification\n",
    "        self.fc1 = nn.Linear(num_layers * dim_embedding, dim_embedding)\n",
    "        self.fc2 = nn.Linear(dim_embedding, dim_target)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x_all = []\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x, edge_index)\n",
    "            if self.aggregation == 'max':\n",
    "                x = torch.relu(self.fc_max(x))\n",
    "            x_all.append(x)\n",
    "\n",
    "        x = torch.cat(x_all, dim=1)\n",
    "        x = global_max_pool(x, batch)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e92f9553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  139521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphSAGE(\n",
       "  (fc_max): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (layers): ModuleList(\n",
       "    (0): SAGEConv(30, 128)\n",
       "    (1): SAGEConv(128, 128)\n",
       "    (2): SAGEConv(128, 128)\n",
       "  )\n",
       "  (fc1): Linear(in_features=384, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_features = train_dataset.num_features\n",
    "dim_target = 1\n",
    "config = {'num_layers': 3, 'dim_embedding': 128, 'aggregation':'max'}\n",
    "model = GraphSAGE(dim_features, dim_target, config)\n",
    "model = model.to(device)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8abbeb",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97f02293",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "178975f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch_item, epoch, batch, training):\n",
    "    #x = batch_item['x'].float()\n",
    "    #edge_index = batch_item['edge_index']\n",
    "    #batch = batch_item['batch']\n",
    "    label = batch_item['y']\n",
    "\n",
    "    if training is True:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(batch_item)\n",
    "            #print(output.shape)\n",
    "            #print(label.shape)\n",
    "            loss = criterion(output.view(-1), label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(batch_item)\n",
    "            #print(output.shape)\n",
    "            #print(label.shape)            \n",
    "            loss = criterion(output.view(-1), label)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "502652da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "380it [00:10, 37.49it/s, Epoch=1, Loss=0.233310, Total Loss=0.294400]\n",
      "95it [00:01, 55.08it/s, Epoch=1, Val Loss=0.204328, Total Val Loss=0.266031]\n",
      "380it [00:08, 42.83it/s, Epoch=2, Loss=0.218151, Total Loss=0.249296]\n",
      "95it [00:01, 65.32it/s, Epoch=2, Val Loss=0.217949, Total Val Loss=0.240808]\n",
      "380it [00:09, 40.14it/s, Epoch=3, Loss=0.256728, Total Loss=0.226953]\n",
      "95it [00:01, 60.05it/s, Epoch=3, Val Loss=0.215080, Total Val Loss=0.224735]\n",
      "380it [00:08, 45.10it/s, Epoch=4, Loss=0.350111, Total Loss=0.212277]\n",
      "95it [00:01, 64.24it/s, Epoch=4, Val Loss=0.195078, Total Val Loss=0.215784]\n",
      "380it [00:08, 42.27it/s, Epoch=5, Loss=0.169847, Total Loss=0.203005]\n",
      "95it [00:01, 48.47it/s, Epoch=5, Val Loss=0.197539, Total Val Loss=0.202190]\n",
      "380it [00:08, 44.88it/s, Epoch=6, Loss=0.216833, Total Loss=0.195042]\n",
      "95it [00:01, 65.33it/s, Epoch=6, Val Loss=0.194557, Total Val Loss=0.199444]\n",
      "380it [00:08, 44.91it/s, Epoch=7, Loss=0.198763, Total Loss=0.191436]\n",
      "95it [00:01, 63.31it/s, Epoch=7, Val Loss=0.228186, Total Val Loss=0.197467]\n",
      "380it [00:08, 45.50it/s, Epoch=8, Loss=0.158364, Total Loss=0.187532]\n",
      "95it [00:01, 58.06it/s, Epoch=8, Val Loss=0.156582, Total Val Loss=0.190897]\n",
      "380it [00:10, 37.36it/s, Epoch=9, Loss=0.224220, Total Loss=0.184056]\n",
      "95it [00:01, 51.46it/s, Epoch=9, Val Loss=0.210864, Total Val Loss=0.190979]\n",
      "380it [00:10, 35.01it/s, Epoch=10, Loss=0.242942, Total Loss=0.181278]\n",
      "95it [00:01, 60.55it/s, Epoch=10, Val Loss=0.197759, Total Val Loss=0.185839]\n",
      "380it [00:09, 39.42it/s, Epoch=11, Loss=0.236736, Total Loss=0.179282]\n",
      "95it [00:01, 62.67it/s, Epoch=11, Val Loss=0.150723, Total Val Loss=0.185297]\n",
      "380it [00:09, 40.49it/s, Epoch=12, Loss=0.141692, Total Loss=0.175554]\n",
      "95it [00:01, 60.36it/s, Epoch=12, Val Loss=0.136412, Total Val Loss=0.182108]\n",
      "380it [00:08, 43.18it/s, Epoch=13, Loss=0.143079, Total Loss=0.174405]\n",
      "95it [00:01, 61.29it/s, Epoch=13, Val Loss=0.190749, Total Val Loss=0.182158]\n",
      "380it [00:08, 45.36it/s, Epoch=14, Loss=0.176277, Total Loss=0.172447]\n",
      "95it [00:01, 52.87it/s, Epoch=14, Val Loss=0.158099, Total Val Loss=0.180168]\n",
      "380it [00:08, 44.52it/s, Epoch=15, Loss=0.107171, Total Loss=0.170819]\n",
      "95it [00:01, 62.19it/s, Epoch=15, Val Loss=0.189260, Total Val Loss=0.180363]\n",
      "380it [00:08, 45.17it/s, Epoch=16, Loss=0.176589, Total Loss=0.169659]\n",
      "95it [00:01, 61.66it/s, Epoch=16, Val Loss=0.178678, Total Val Loss=0.180208]\n",
      "380it [00:08, 45.10it/s, Epoch=17, Loss=0.182574, Total Loss=0.167750]\n",
      "95it [00:01, 59.49it/s, Epoch=17, Val Loss=0.206669, Total Val Loss=0.176978]\n",
      "380it [00:08, 45.64it/s, Epoch=18, Loss=0.133372, Total Loss=0.168567]\n",
      "95it [00:01, 62.44it/s, Epoch=18, Val Loss=0.137691, Total Val Loss=0.175818]\n",
      "380it [00:08, 44.91it/s, Epoch=19, Loss=0.166191, Total Loss=0.165058]\n",
      "95it [00:01, 63.34it/s, Epoch=19, Val Loss=0.191120, Total Val Loss=0.181898]\n",
      "380it [00:08, 45.56it/s, Epoch=20, Loss=0.219719, Total Loss=0.163932]\n",
      "95it [00:01, 64.16it/s, Epoch=20, Val Loss=0.184254, Total Val Loss=0.177509]\n",
      "380it [00:08, 45.49it/s, Epoch=21, Loss=0.168986, Total Loss=0.162480]\n",
      "95it [00:01, 47.98it/s, Epoch=21, Val Loss=0.171790, Total Val Loss=0.178515]\n",
      "380it [00:08, 44.07it/s, Epoch=22, Loss=0.202918, Total Loss=0.161480]\n",
      "95it [00:01, 58.87it/s, Epoch=22, Val Loss=0.156515, Total Val Loss=0.176040]\n",
      "380it [00:08, 44.60it/s, Epoch=23, Loss=0.139533, Total Loss=0.160196]\n",
      "95it [00:01, 66.27it/s, Epoch=23, Val Loss=0.196232, Total Val Loss=0.172710]\n",
      "380it [00:08, 45.10it/s, Epoch=24, Loss=0.140679, Total Loss=0.158873]\n",
      "95it [00:01, 62.38it/s, Epoch=24, Val Loss=0.174463, Total Val Loss=0.178279]\n",
      "380it [00:08, 44.44it/s, Epoch=25, Loss=0.217619, Total Loss=0.158711]\n",
      "95it [00:01, 64.64it/s, Epoch=25, Val Loss=0.147418, Total Val Loss=0.169928]\n",
      "380it [00:08, 44.23it/s, Epoch=26, Loss=0.193509, Total Loss=0.156098]\n",
      "95it [00:01, 58.76it/s, Epoch=26, Val Loss=0.139631, Total Val Loss=0.169489]\n",
      "380it [00:08, 44.67it/s, Epoch=27, Loss=0.192875, Total Loss=0.155901]\n",
      "95it [00:01, 64.51it/s, Epoch=27, Val Loss=0.200979, Total Val Loss=0.184809]\n",
      "380it [00:08, 45.48it/s, Epoch=28, Loss=0.144462, Total Loss=0.154255]\n",
      "95it [00:01, 58.19it/s, Epoch=28, Val Loss=0.189127, Total Val Loss=0.171900]\n",
      "380it [00:08, 42.62it/s, Epoch=29, Loss=0.176596, Total Loss=0.153267]\n",
      "95it [00:01, 54.06it/s, Epoch=29, Val Loss=0.196518, Total Val Loss=0.168374]\n",
      "380it [00:08, 42.72it/s, Epoch=30, Loss=0.187875, Total Loss=0.151833]\n",
      "95it [00:01, 56.91it/s, Epoch=30, Val Loss=0.183885, Total Val Loss=0.168633]\n",
      "380it [00:10, 36.58it/s, Epoch=31, Loss=0.129335, Total Loss=0.151398]\n",
      "95it [00:01, 55.06it/s, Epoch=31, Val Loss=0.156264, Total Val Loss=0.166912]\n",
      "380it [00:08, 43.75it/s, Epoch=32, Loss=0.126235, Total Loss=0.150263]\n",
      "95it [00:01, 54.54it/s, Epoch=32, Val Loss=0.168564, Total Val Loss=0.167110]\n",
      "380it [00:08, 45.67it/s, Epoch=33, Loss=0.128168, Total Loss=0.150071]\n",
      "95it [00:01, 62.66it/s, Epoch=33, Val Loss=0.185116, Total Val Loss=0.165307]\n",
      "380it [00:08, 44.13it/s, Epoch=34, Loss=0.180288, Total Loss=0.147816]\n",
      "95it [00:01, 64.94it/s, Epoch=34, Val Loss=0.162993, Total Val Loss=0.165684]\n",
      "380it [00:08, 45.45it/s, Epoch=35, Loss=0.176598, Total Loss=0.147001]\n",
      "95it [00:01, 65.79it/s, Epoch=35, Val Loss=0.124482, Total Val Loss=0.165237]\n",
      "380it [00:08, 43.67it/s, Epoch=36, Loss=0.132766, Total Loss=0.146913]\n",
      "95it [00:01, 52.99it/s, Epoch=36, Val Loss=0.174436, Total Val Loss=0.164838]\n",
      "380it [00:09, 41.22it/s, Epoch=37, Loss=0.161111, Total Loss=0.144957]\n",
      "95it [00:01, 49.18it/s, Epoch=37, Val Loss=0.193062, Total Val Loss=0.163836]\n",
      "380it [00:09, 41.87it/s, Epoch=38, Loss=0.131405, Total Loss=0.143842]\n",
      "95it [00:01, 48.95it/s, Epoch=38, Val Loss=0.153332, Total Val Loss=0.163006]\n",
      "380it [00:08, 44.94it/s, Epoch=39, Loss=0.149778, Total Loss=0.144645]\n",
      "95it [00:01, 54.47it/s, Epoch=39, Val Loss=0.186822, Total Val Loss=0.162364]\n",
      "380it [00:08, 44.67it/s, Epoch=40, Loss=0.107356, Total Loss=0.141829]\n",
      "95it [00:01, 56.31it/s, Epoch=40, Val Loss=0.161655, Total Val Loss=0.161259]\n",
      "380it [00:09, 41.88it/s, Epoch=41, Loss=0.147834, Total Loss=0.142385]\n",
      "95it [00:01, 52.99it/s, Epoch=41, Val Loss=0.197264, Total Val Loss=0.165274]\n",
      "380it [00:08, 45.32it/s, Epoch=42, Loss=0.135184, Total Loss=0.140798]\n",
      "95it [00:01, 52.87it/s, Epoch=42, Val Loss=0.155706, Total Val Loss=0.161122]\n",
      "380it [00:08, 44.93it/s, Epoch=43, Loss=0.093666, Total Loss=0.140714]\n",
      "95it [00:01, 53.14it/s, Epoch=43, Val Loss=0.131608, Total Val Loss=0.161534]\n",
      "380it [00:09, 40.13it/s, Epoch=44, Loss=0.107953, Total Loss=0.138604]\n",
      "95it [00:01, 55.10it/s, Epoch=44, Val Loss=0.164190, Total Val Loss=0.159116]\n",
      "380it [00:08, 43.51it/s, Epoch=45, Loss=0.108446, Total Loss=0.138485]\n",
      "95it [00:01, 54.37it/s, Epoch=45, Val Loss=0.169308, Total Val Loss=0.160472]\n",
      "380it [00:09, 40.98it/s, Epoch=46, Loss=0.132948, Total Loss=0.137273]\n",
      "95it [00:01, 54.99it/s, Epoch=46, Val Loss=0.156085, Total Val Loss=0.161227]\n",
      "380it [00:09, 39.70it/s, Epoch=47, Loss=0.070778, Total Loss=0.135968]\n",
      "95it [00:01, 49.65it/s, Epoch=47, Val Loss=0.185246, Total Val Loss=0.172600]\n",
      "380it [00:08, 43.91it/s, Epoch=48, Loss=0.082175, Total Loss=0.135707]\n",
      "95it [00:01, 60.12it/s, Epoch=48, Val Loss=0.146914, Total Val Loss=0.161529]\n",
      "380it [00:08, 44.14it/s, Epoch=49, Loss=0.090636, Total Loss=0.134596]\n",
      "95it [00:01, 53.97it/s, Epoch=49, Val Loss=0.126462, Total Val Loss=0.157403]\n",
      "380it [00:09, 38.32it/s, Epoch=50, Loss=0.094807, Total Loss=0.133063]\n",
      "95it [00:01, 52.82it/s, Epoch=50, Val Loss=0.164526, Total Val Loss=0.158970]\n",
      "380it [00:08, 43.90it/s, Epoch=51, Loss=0.132351, Total Loss=0.133568]\n",
      "95it [00:01, 63.39it/s, Epoch=51, Val Loss=0.194025, Total Val Loss=0.163767]\n",
      "380it [00:08, 45.33it/s, Epoch=52, Loss=0.080340, Total Loss=0.132553]\n",
      "95it [00:01, 63.41it/s, Epoch=52, Val Loss=0.146115, Total Val Loss=0.160676]\n",
      "380it [00:08, 43.63it/s, Epoch=53, Loss=0.138643, Total Loss=0.132357]\n",
      "95it [00:01, 53.00it/s, Epoch=53, Val Loss=0.168767, Total Val Loss=0.162113]\n",
      "380it [00:08, 44.09it/s, Epoch=54, Loss=0.126672, Total Loss=0.131754]\n",
      "95it [00:01, 56.97it/s, Epoch=54, Val Loss=0.118105, Total Val Loss=0.158001]\n",
      "380it [00:08, 43.26it/s, Epoch=55, Loss=0.119068, Total Loss=0.131134]\n",
      "95it [00:01, 49.31it/s, Epoch=55, Val Loss=0.132611, Total Val Loss=0.163069]\n",
      "380it [00:09, 41.31it/s, Epoch=56, Loss=0.114173, Total Loss=0.128775]\n",
      "95it [00:01, 49.20it/s, Epoch=56, Val Loss=0.144156, Total Val Loss=0.156605]\n",
      "380it [00:09, 41.70it/s, Epoch=57, Loss=0.094390, Total Loss=0.128498]\n",
      "95it [00:01, 49.64it/s, Epoch=57, Val Loss=0.191205, Total Val Loss=0.156216]\n",
      "380it [00:09, 39.31it/s, Epoch=58, Loss=0.125615, Total Loss=0.128809]\n",
      "95it [00:01, 51.72it/s, Epoch=58, Val Loss=0.152853, Total Val Loss=0.156361]\n",
      "380it [00:09, 39.81it/s, Epoch=59, Loss=0.090369, Total Loss=0.126787]\n",
      "95it [00:01, 50.26it/s, Epoch=59, Val Loss=0.187909, Total Val Loss=0.157463]\n",
      "380it [00:08, 44.72it/s, Epoch=60, Loss=0.122092, Total Loss=0.126810]\n",
      "95it [00:01, 55.47it/s, Epoch=60, Val Loss=0.137081, Total Val Loss=0.161009]\n",
      "380it [00:08, 44.74it/s, Epoch=61, Loss=0.133563, Total Loss=0.128708]\n",
      "95it [00:01, 54.05it/s, Epoch=61, Val Loss=0.163376, Total Val Loss=0.156178]\n",
      "380it [00:08, 42.24it/s, Epoch=62, Loss=0.069833, Total Loss=0.125466]\n",
      "95it [00:01, 52.21it/s, Epoch=62, Val Loss=0.159601, Total Val Loss=0.156828]\n",
      "380it [00:11, 33.83it/s, Epoch=63, Loss=0.115821, Total Loss=0.125725]\n",
      "95it [00:01, 55.22it/s, Epoch=63, Val Loss=0.173723, Total Val Loss=0.162009]\n",
      "380it [00:08, 44.62it/s, Epoch=64, Loss=0.091120, Total Loss=0.124608]\n",
      "95it [00:01, 59.17it/s, Epoch=64, Val Loss=0.140873, Total Val Loss=0.156817]\n",
      "380it [00:09, 38.27it/s, Epoch=65, Loss=0.087241, Total Loss=0.124184]\n",
      "95it [00:01, 55.22it/s, Epoch=65, Val Loss=0.131716, Total Val Loss=0.159085]\n",
      "380it [00:09, 42.15it/s, Epoch=66, Loss=0.077676, Total Loss=0.123309]\n",
      "95it [00:01, 61.30it/s, Epoch=66, Val Loss=0.145020, Total Val Loss=0.155544]\n",
      "380it [00:10, 35.26it/s, Epoch=67, Loss=0.136516, Total Loss=0.123628]\n",
      "95it [00:01, 58.66it/s, Epoch=67, Val Loss=0.144028, Total Val Loss=0.157865]\n",
      "380it [00:08, 43.13it/s, Epoch=68, Loss=0.107870, Total Loss=0.121230]\n",
      "95it [00:01, 53.92it/s, Epoch=68, Val Loss=0.159872, Total Val Loss=0.156397]\n",
      "380it [00:09, 41.74it/s, Epoch=69, Loss=0.208221, Total Loss=0.121970]\n",
      "95it [00:01, 52.01it/s, Epoch=69, Val Loss=0.145949, Total Val Loss=0.157725]\n",
      "380it [00:08, 43.55it/s, Epoch=70, Loss=0.196006, Total Loss=0.120978]\n",
      "95it [00:01, 56.73it/s, Epoch=70, Val Loss=0.160070, Total Val Loss=0.162266]\n",
      "380it [00:08, 44.63it/s, Epoch=71, Loss=0.134668, Total Loss=0.119582]\n",
      "95it [00:01, 53.22it/s, Epoch=71, Val Loss=0.156998, Total Val Loss=0.154805]\n",
      "380it [00:08, 42.33it/s, Epoch=72, Loss=0.090678, Total Loss=0.118801]\n",
      "95it [00:01, 53.44it/s, Epoch=72, Val Loss=0.187863, Total Val Loss=0.155198]\n",
      "380it [00:10, 37.79it/s, Epoch=73, Loss=0.129271, Total Loss=0.118875]\n",
      "95it [00:01, 49.46it/s, Epoch=73, Val Loss=0.176379, Total Val Loss=0.157954]\n",
      "380it [00:08, 43.89it/s, Epoch=74, Loss=0.141833, Total Loss=0.119818]\n",
      "95it [00:01, 60.75it/s, Epoch=74, Val Loss=0.152392, Total Val Loss=0.156507]\n",
      "380it [00:08, 45.06it/s, Epoch=75, Loss=0.099661, Total Loss=0.118184]\n",
      "95it [00:01, 51.37it/s, Epoch=75, Val Loss=0.162190, Total Val Loss=0.163796]\n",
      "380it [00:09, 39.45it/s, Epoch=76, Loss=0.092258, Total Loss=0.117707]\n",
      "95it [00:01, 56.50it/s, Epoch=76, Val Loss=0.150210, Total Val Loss=0.158216]\n",
      "380it [00:08, 43.27it/s, Epoch=77, Loss=0.118374, Total Loss=0.116693]\n",
      "95it [00:01, 58.67it/s, Epoch=77, Val Loss=0.156661, Total Val Loss=0.156503]\n",
      "380it [00:08, 44.61it/s, Epoch=78, Loss=0.080822, Total Loss=0.116650]\n",
      "95it [00:01, 57.29it/s, Epoch=78, Val Loss=0.166087, Total Val Loss=0.155152]\n",
      "380it [00:08, 43.71it/s, Epoch=79, Loss=0.116940, Total Loss=0.116022]\n",
      "95it [00:01, 54.30it/s, Epoch=79, Val Loss=0.173821, Total Val Loss=0.155720]\n",
      "380it [00:09, 38.30it/s, Epoch=80, Loss=0.071510, Total Loss=0.115420]\n",
      "95it [00:01, 54.76it/s, Epoch=80, Val Loss=0.141851, Total Val Loss=0.156937]\n",
      "380it [00:08, 43.15it/s, Epoch=81, Loss=0.069241, Total Loss=0.114735]\n",
      "95it [00:01, 52.25it/s, Epoch=81, Val Loss=0.182542, Total Val Loss=0.155875]\n",
      "380it [00:10, 36.29it/s, Epoch=82, Loss=0.117670, Total Loss=0.114535]\n",
      "95it [00:01, 56.96it/s, Epoch=82, Val Loss=0.210765, Total Val Loss=0.158343]\n",
      "380it [00:11, 31.70it/s, Epoch=83, Loss=0.111495, Total Loss=0.113128]\n",
      "95it [00:01, 56.77it/s, Epoch=83, Val Loss=0.134270, Total Val Loss=0.155694]\n",
      "380it [00:09, 40.81it/s, Epoch=84, Loss=0.089965, Total Loss=0.114673]\n",
      "95it [00:01, 54.07it/s, Epoch=84, Val Loss=0.128294, Total Val Loss=0.154991]\n",
      "380it [00:08, 44.75it/s, Epoch=85, Loss=0.108568, Total Loss=0.113094]\n",
      "95it [00:01, 50.47it/s, Epoch=85, Val Loss=0.120607, Total Val Loss=0.155857]\n",
      "380it [00:08, 43.76it/s, Epoch=86, Loss=0.173260, Total Loss=0.111584]\n",
      "95it [00:01, 54.38it/s, Epoch=86, Val Loss=0.215980, Total Val Loss=0.157668]\n",
      "380it [00:09, 39.52it/s, Epoch=87, Loss=0.106517, Total Loss=0.111543]\n",
      "95it [00:01, 53.37it/s, Epoch=87, Val Loss=0.156636, Total Val Loss=0.155836]\n",
      "380it [00:10, 34.62it/s, Epoch=88, Loss=0.177947, Total Loss=0.111653]\n",
      "95it [00:01, 52.03it/s, Epoch=88, Val Loss=0.183744, Total Val Loss=0.159283]\n",
      "380it [00:08, 44.04it/s, Epoch=89, Loss=0.127764, Total Loss=0.111152]\n",
      "95it [00:01, 56.20it/s, Epoch=89, Val Loss=0.183650, Total Val Loss=0.160816]\n",
      "380it [00:08, 45.51it/s, Epoch=90, Loss=0.106213, Total Loss=0.110200]\n",
      "95it [00:01, 56.55it/s, Epoch=90, Val Loss=0.155274, Total Val Loss=0.163383]\n",
      "380it [00:08, 44.41it/s, Epoch=91, Loss=0.126007, Total Loss=0.109748]\n",
      "95it [00:01, 58.26it/s, Epoch=91, Val Loss=0.151721, Total Val Loss=0.157067]\n",
      "380it [00:08, 43.11it/s, Epoch=92, Loss=0.090000, Total Loss=0.108587]\n",
      "95it [00:01, 55.04it/s, Epoch=92, Val Loss=0.195993, Total Val Loss=0.154614]\n",
      "380it [00:09, 40.02it/s, Epoch=93, Loss=0.100675, Total Loss=0.107761]\n",
      "95it [00:01, 50.58it/s, Epoch=93, Val Loss=0.125850, Total Val Loss=0.158228]\n",
      "380it [00:08, 43.04it/s, Epoch=94, Loss=0.129290, Total Loss=0.110559]\n",
      "95it [00:01, 54.19it/s, Epoch=94, Val Loss=0.126753, Total Val Loss=0.155298]\n",
      "380it [00:08, 45.10it/s, Epoch=95, Loss=0.105128, Total Loss=0.107327]\n",
      "95it [00:01, 53.22it/s, Epoch=95, Val Loss=0.162288, Total Val Loss=0.158245]\n",
      "380it [00:08, 45.01it/s, Epoch=96, Loss=0.099429, Total Loss=0.106424]\n",
      "95it [00:01, 55.66it/s, Epoch=96, Val Loss=0.114779, Total Val Loss=0.156496]\n",
      "380it [00:08, 42.48it/s, Epoch=97, Loss=0.079946, Total Loss=0.106806]\n",
      "95it [00:01, 54.46it/s, Epoch=97, Val Loss=0.144483, Total Val Loss=0.155121]\n",
      "380it [00:08, 42.33it/s, Epoch=98, Loss=0.094377, Total Loss=0.106825]\n",
      "95it [00:01, 56.56it/s, Epoch=98, Val Loss=0.122606, Total Val Loss=0.158333]\n",
      "380it [00:09, 41.66it/s, Epoch=99, Loss=0.164536, Total Loss=0.106556]\n",
      "95it [00:01, 53.57it/s, Epoch=99, Val Loss=0.129566, Total Val Loss=0.156834]\n",
      "380it [00:08, 43.64it/s, Epoch=100, Loss=0.084858, Total Loss=0.105572]\n",
      "95it [00:01, 55.49it/s, Epoch=100, Val Loss=0.149507, Total Val Loss=0.159704]\n",
      "380it [00:09, 40.52it/s, Epoch=101, Loss=0.093945, Total Loss=0.104523]\n",
      "95it [00:01, 51.54it/s, Epoch=101, Val Loss=0.130824, Total Val Loss=0.156487]\n",
      "380it [00:10, 36.48it/s, Epoch=102, Loss=0.085867, Total Loss=0.102923]\n",
      "95it [00:01, 52.60it/s, Epoch=102, Val Loss=0.180172, Total Val Loss=0.154785]\n",
      "380it [00:09, 39.54it/s, Epoch=103, Loss=0.097692, Total Loss=0.104518]\n",
      "95it [00:01, 59.67it/s, Epoch=103, Val Loss=0.126255, Total Val Loss=0.155060]\n",
      "380it [00:09, 42.20it/s, Epoch=104, Loss=0.076318, Total Loss=0.103411]\n",
      "95it [00:01, 57.54it/s, Epoch=104, Val Loss=0.162391, Total Val Loss=0.156815]\n",
      "380it [00:08, 44.09it/s, Epoch=105, Loss=0.102191, Total Loss=0.102856]\n",
      "95it [00:01, 60.36it/s, Epoch=105, Val Loss=0.158423, Total Val Loss=0.156555]\n",
      "380it [00:08, 44.96it/s, Epoch=106, Loss=0.048039, Total Loss=0.101549]\n",
      "95it [00:01, 49.09it/s, Epoch=106, Val Loss=0.174054, Total Val Loss=0.161673]\n",
      "380it [00:08, 43.49it/s, Epoch=107, Loss=0.091425, Total Loss=0.102268]\n",
      "95it [00:01, 61.79it/s, Epoch=107, Val Loss=0.166333, Total Val Loss=0.156795]\n",
      "380it [00:09, 38.84it/s, Epoch=108, Loss=0.110113, Total Loss=0.101974]\n",
      "95it [00:01, 50.56it/s, Epoch=108, Val Loss=0.191144, Total Val Loss=0.155421]\n",
      "380it [00:08, 42.34it/s, Epoch=109, Loss=0.065535, Total Loss=0.101431]\n",
      "95it [00:01, 53.21it/s, Epoch=109, Val Loss=0.162836, Total Val Loss=0.156261]\n",
      "380it [00:09, 39.11it/s, Epoch=110, Loss=0.081350, Total Loss=0.101595]\n",
      "95it [00:01, 53.79it/s, Epoch=110, Val Loss=0.166012, Total Val Loss=0.156827]\n",
      "380it [00:08, 43.97it/s, Epoch=111, Loss=0.129602, Total Loss=0.101041]\n",
      "95it [00:01, 63.47it/s, Epoch=111, Val Loss=0.156013, Total Val Loss=0.155476]\n",
      "380it [00:08, 44.03it/s, Epoch=112, Loss=0.182415, Total Loss=0.099972]\n",
      "95it [00:01, 59.16it/s, Epoch=112, Val Loss=0.156422, Total Val Loss=0.160709]\n",
      "380it [00:09, 40.04it/s, Epoch=113, Loss=0.092723, Total Loss=0.100732]\n",
      "95it [00:01, 55.18it/s, Epoch=113, Val Loss=0.127654, Total Val Loss=0.156633]\n",
      "380it [00:08, 44.60it/s, Epoch=114, Loss=0.073358, Total Loss=0.100654]\n",
      "95it [00:01, 51.89it/s, Epoch=114, Val Loss=0.174411, Total Val Loss=0.157080]\n",
      "380it [00:09, 41.67it/s, Epoch=115, Loss=0.112912, Total Loss=0.098891]\n",
      "95it [00:01, 60.15it/s, Epoch=115, Val Loss=0.183407, Total Val Loss=0.163661]\n",
      "380it [00:09, 40.72it/s, Epoch=116, Loss=0.097384, Total Loss=0.098782]\n",
      "95it [00:01, 57.77it/s, Epoch=116, Val Loss=0.145760, Total Val Loss=0.156443]\n",
      "380it [00:09, 40.60it/s, Epoch=117, Loss=0.113298, Total Loss=0.098845]\n",
      "95it [00:01, 58.28it/s, Epoch=117, Val Loss=0.138625, Total Val Loss=0.156825]\n",
      "380it [00:09, 41.95it/s, Epoch=118, Loss=0.123353, Total Loss=0.097980]\n",
      "95it [00:01, 55.77it/s, Epoch=118, Val Loss=0.122465, Total Val Loss=0.157173]\n",
      "380it [00:08, 42.70it/s, Epoch=119, Loss=0.052633, Total Loss=0.097623]\n",
      "95it [00:01, 63.06it/s, Epoch=119, Val Loss=0.171348, Total Val Loss=0.163267]\n",
      "380it [00:09, 41.80it/s, Epoch=120, Loss=0.108687, Total Loss=0.099224]\n",
      "95it [00:01, 64.40it/s, Epoch=120, Val Loss=0.138523, Total Val Loss=0.159762]\n",
      "380it [00:08, 45.12it/s, Epoch=121, Loss=0.103735, Total Loss=0.097380]\n",
      "95it [00:01, 61.67it/s, Epoch=121, Val Loss=0.167192, Total Val Loss=0.170833]\n",
      "380it [00:08, 44.51it/s, Epoch=122, Loss=0.119183, Total Loss=0.096634]\n",
      "95it [00:01, 54.47it/s, Epoch=122, Val Loss=0.172253, Total Val Loss=0.159402]\n",
      "380it [00:08, 43.78it/s, Epoch=123, Loss=0.074941, Total Loss=0.095739]\n",
      "95it [00:01, 59.08it/s, Epoch=123, Val Loss=0.133030, Total Val Loss=0.158607]\n",
      "380it [00:10, 37.29it/s, Epoch=124, Loss=0.095125, Total Loss=0.094934]\n",
      "95it [00:01, 58.67it/s, Epoch=124, Val Loss=0.172869, Total Val Loss=0.160290]\n",
      "380it [00:10, 35.19it/s, Epoch=125, Loss=0.079094, Total Loss=0.094746]\n",
      "95it [00:01, 51.85it/s, Epoch=125, Val Loss=0.186358, Total Val Loss=0.169519]\n",
      "380it [00:09, 38.05it/s, Epoch=126, Loss=0.101837, Total Loss=0.095438]\n",
      "95it [00:01, 59.58it/s, Epoch=126, Val Loss=0.193460, Total Val Loss=0.158265]\n",
      "380it [00:08, 44.88it/s, Epoch=127, Loss=0.138391, Total Loss=0.094518]\n",
      "95it [00:01, 54.42it/s, Epoch=127, Val Loss=0.152852, Total Val Loss=0.156095]\n",
      "380it [00:09, 40.50it/s, Epoch=128, Loss=0.058207, Total Loss=0.093461]\n",
      "95it [00:01, 54.98it/s, Epoch=128, Val Loss=0.169223, Total Val Loss=0.160325]\n",
      "380it [00:08, 43.80it/s, Epoch=129, Loss=0.071340, Total Loss=0.093085]\n",
      "95it [00:01, 60.51it/s, Epoch=129, Val Loss=0.145646, Total Val Loss=0.159439]\n",
      "380it [00:08, 43.47it/s, Epoch=130, Loss=0.074669, Total Loss=0.093537]\n",
      "95it [00:01, 57.67it/s, Epoch=130, Val Loss=0.181727, Total Val Loss=0.157473]\n",
      "380it [00:08, 44.98it/s, Epoch=131, Loss=0.073812, Total Loss=0.092990]\n",
      "95it [00:01, 58.11it/s, Epoch=131, Val Loss=0.134014, Total Val Loss=0.156012]\n",
      "380it [00:08, 44.90it/s, Epoch=132, Loss=0.056089, Total Loss=0.091966]\n",
      "95it [00:01, 57.84it/s, Epoch=132, Val Loss=0.164078, Total Val Loss=0.157616]\n",
      "380it [00:09, 41.79it/s, Epoch=133, Loss=0.086573, Total Loss=0.091852]\n",
      "95it [00:01, 60.05it/s, Epoch=133, Val Loss=0.129349, Total Val Loss=0.163197]\n",
      "380it [00:08, 43.77it/s, Epoch=134, Loss=0.090042, Total Loss=0.091905]\n",
      "95it [00:01, 60.07it/s, Epoch=134, Val Loss=0.128189, Total Val Loss=0.157535]\n",
      "380it [00:08, 44.78it/s, Epoch=135, Loss=0.074054, Total Loss=0.090829]\n",
      "95it [00:01, 62.17it/s, Epoch=135, Val Loss=0.148959, Total Val Loss=0.158358]\n",
      "380it [00:08, 44.58it/s, Epoch=136, Loss=0.097283, Total Loss=0.090790]\n",
      "95it [00:01, 60.39it/s, Epoch=136, Val Loss=0.159035, Total Val Loss=0.161916]\n",
      "380it [00:08, 44.72it/s, Epoch=137, Loss=0.064382, Total Loss=0.092948]\n",
      "95it [00:01, 60.80it/s, Epoch=137, Val Loss=0.168661, Total Val Loss=0.157780]\n",
      "380it [00:08, 44.52it/s, Epoch=138, Loss=0.124247, Total Loss=0.089924]\n",
      "95it [00:01, 63.40it/s, Epoch=138, Val Loss=0.190767, Total Val Loss=0.161175]\n",
      "380it [00:08, 45.23it/s, Epoch=139, Loss=0.055777, Total Loss=0.090703]\n",
      "95it [00:01, 62.72it/s, Epoch=139, Val Loss=0.171140, Total Val Loss=0.177264]\n",
      "380it [00:08, 44.22it/s, Epoch=140, Loss=0.145349, Total Loss=0.089916]\n",
      "95it [00:01, 62.63it/s, Epoch=140, Val Loss=0.172797, Total Val Loss=0.160439]\n",
      "380it [00:08, 43.54it/s, Epoch=141, Loss=0.107135, Total Loss=0.088940]\n",
      "95it [00:01, 65.26it/s, Epoch=141, Val Loss=0.164444, Total Val Loss=0.157052]\n",
      "380it [00:08, 42.97it/s, Epoch=142, Loss=0.064167, Total Loss=0.089121]\n",
      "95it [00:01, 65.27it/s, Epoch=142, Val Loss=0.160038, Total Val Loss=0.157699]\n",
      "380it [00:08, 44.70it/s, Epoch=143, Loss=0.044833, Total Loss=0.087739]\n",
      "95it [00:01, 52.95it/s, Epoch=143, Val Loss=0.153794, Total Val Loss=0.159767]\n",
      "380it [00:08, 44.54it/s, Epoch=144, Loss=0.072612, Total Loss=0.088315]\n",
      "95it [00:01, 52.83it/s, Epoch=144, Val Loss=0.180299, Total Val Loss=0.157935]\n",
      "380it [00:08, 43.67it/s, Epoch=145, Loss=0.115716, Total Loss=0.089294]\n",
      "95it [00:01, 51.83it/s, Epoch=145, Val Loss=0.146524, Total Val Loss=0.158759]\n",
      "380it [00:08, 44.08it/s, Epoch=146, Loss=0.091350, Total Loss=0.087617]\n",
      "95it [00:01, 52.47it/s, Epoch=146, Val Loss=0.170189, Total Val Loss=0.155984]\n",
      "380it [00:09, 42.21it/s, Epoch=147, Loss=0.114398, Total Loss=0.087191]\n",
      "95it [00:01, 63.27it/s, Epoch=147, Val Loss=0.193449, Total Val Loss=0.158491]\n",
      "380it [00:08, 43.86it/s, Epoch=148, Loss=0.054025, Total Loss=0.089283]\n",
      "95it [00:01, 59.67it/s, Epoch=148, Val Loss=0.177252, Total Val Loss=0.157826]\n",
      "380it [00:08, 43.96it/s, Epoch=149, Loss=0.108874, Total Loss=0.087609]\n",
      "95it [00:01, 63.90it/s, Epoch=149, Val Loss=0.169432, Total Val Loss=0.157395]\n",
      "380it [00:08, 44.74it/s, Epoch=150, Loss=0.058797, Total Loss=0.086687]\n",
      "95it [00:01, 53.92it/s, Epoch=150, Val Loss=0.149921, Total Val Loss=0.156465]\n",
      "380it [00:08, 43.33it/s, Epoch=151, Loss=0.157674, Total Loss=0.086458]\n",
      "95it [00:01, 51.76it/s, Epoch=151, Val Loss=0.154864, Total Val Loss=0.158378]\n",
      "380it [00:10, 36.51it/s, Epoch=152, Loss=0.075003, Total Loss=0.086555]\n",
      "95it [00:01, 58.72it/s, Epoch=152, Val Loss=0.176778, Total Val Loss=0.157587]\n",
      "380it [00:10, 36.95it/s, Epoch=153, Loss=0.060658, Total Loss=0.085040]\n",
      "95it [00:01, 49.79it/s, Epoch=153, Val Loss=0.185103, Total Val Loss=0.159778]\n",
      "380it [00:08, 45.03it/s, Epoch=154, Loss=0.084317, Total Loss=0.085503]\n",
      "95it [00:01, 54.02it/s, Epoch=154, Val Loss=0.154243, Total Val Loss=0.159903]\n",
      "380it [00:09, 41.18it/s, Epoch=155, Loss=0.056663, Total Loss=0.084628]\n",
      "95it [00:01, 63.86it/s, Epoch=155, Val Loss=0.179560, Total Val Loss=0.158569]\n",
      "380it [00:08, 45.10it/s, Epoch=156, Loss=0.102270, Total Loss=0.083571]\n",
      "95it [00:01, 54.65it/s, Epoch=156, Val Loss=0.161947, Total Val Loss=0.159449]\n",
      "380it [00:08, 44.13it/s, Epoch=157, Loss=0.072485, Total Loss=0.083833]\n",
      "95it [00:01, 53.93it/s, Epoch=157, Val Loss=0.159660, Total Val Loss=0.158490]\n",
      "380it [00:08, 44.93it/s, Epoch=158, Loss=0.065365, Total Loss=0.084294]\n",
      "95it [00:01, 59.48it/s, Epoch=158, Val Loss=0.176176, Total Val Loss=0.158165]\n",
      "380it [00:08, 43.79it/s, Epoch=159, Loss=0.106873, Total Loss=0.084370]\n",
      "95it [00:01, 65.31it/s, Epoch=159, Val Loss=0.129128, Total Val Loss=0.156897]\n",
      "380it [00:08, 44.58it/s, Epoch=160, Loss=0.117804, Total Loss=0.083759]\n",
      "95it [00:01, 63.12it/s, Epoch=160, Val Loss=0.149190, Total Val Loss=0.158361]\n",
      "380it [00:09, 41.74it/s, Epoch=161, Loss=0.048600, Total Loss=0.083922]\n",
      "95it [00:01, 51.26it/s, Epoch=161, Val Loss=0.144627, Total Val Loss=0.164438]\n",
      "380it [00:10, 36.92it/s, Epoch=162, Loss=0.073849, Total Loss=0.083809]\n",
      "95it [00:01, 52.17it/s, Epoch=162, Val Loss=0.152229, Total Val Loss=0.158227]\n",
      "380it [00:09, 41.76it/s, Epoch=163, Loss=0.140318, Total Loss=0.081863]\n",
      "95it [00:01, 51.09it/s, Epoch=163, Val Loss=0.175529, Total Val Loss=0.160309]\n",
      "380it [00:09, 39.96it/s, Epoch=164, Loss=0.056414, Total Loss=0.082158]\n",
      "95it [00:01, 56.26it/s, Epoch=164, Val Loss=0.204308, Total Val Loss=0.157628]\n",
      "380it [00:08, 43.03it/s, Epoch=165, Loss=0.095435, Total Loss=0.082660]\n",
      "95it [00:01, 50.23it/s, Epoch=165, Val Loss=0.152772, Total Val Loss=0.160710]\n",
      "380it [00:09, 39.30it/s, Epoch=166, Loss=0.108266, Total Loss=0.081899]\n",
      "95it [00:01, 58.88it/s, Epoch=166, Val Loss=0.146969, Total Val Loss=0.161336]\n",
      "380it [00:10, 36.28it/s, Epoch=167, Loss=0.119387, Total Loss=0.083422]\n",
      "95it [00:01, 51.65it/s, Epoch=167, Val Loss=0.163373, Total Val Loss=0.162712]\n",
      "380it [00:08, 42.23it/s, Epoch=168, Loss=0.060466, Total Loss=0.081155]\n",
      "95it [00:01, 55.68it/s, Epoch=168, Val Loss=0.132468, Total Val Loss=0.159101]\n",
      "380it [00:08, 44.51it/s, Epoch=169, Loss=0.081354, Total Loss=0.080194]\n",
      "95it [00:01, 53.80it/s, Epoch=169, Val Loss=0.178290, Total Val Loss=0.158097]\n",
      "380it [00:08, 42.50it/s, Epoch=170, Loss=0.052266, Total Loss=0.081565]\n",
      "95it [00:01, 51.01it/s, Epoch=170, Val Loss=0.146280, Total Val Loss=0.159324]\n",
      "380it [00:08, 45.07it/s, Epoch=171, Loss=0.050123, Total Loss=0.079941]\n",
      "95it [00:01, 54.39it/s, Epoch=171, Val Loss=0.128813, Total Val Loss=0.157372]\n",
      "380it [00:08, 43.98it/s, Epoch=172, Loss=0.059611, Total Loss=0.080866]\n",
      "95it [00:01, 61.55it/s, Epoch=172, Val Loss=0.178023, Total Val Loss=0.157988]\n",
      "380it [00:09, 40.69it/s, Epoch=173, Loss=0.094589, Total Loss=0.079945]\n",
      "95it [00:01, 54.97it/s, Epoch=173, Val Loss=0.178348, Total Val Loss=0.158126]\n",
      "380it [00:10, 37.48it/s, Epoch=174, Loss=0.105662, Total Loss=0.079261]\n",
      "95it [00:02, 44.51it/s, Epoch=174, Val Loss=0.162408, Total Val Loss=0.161197]\n",
      "380it [00:08, 44.34it/s, Epoch=175, Loss=0.061144, Total Loss=0.078726]\n",
      "95it [00:01, 54.59it/s, Epoch=175, Val Loss=0.161007, Total Val Loss=0.162876]\n",
      "380it [00:09, 40.45it/s, Epoch=176, Loss=0.066582, Total Loss=0.079846]\n",
      "95it [00:01, 53.52it/s, Epoch=176, Val Loss=0.155527, Total Val Loss=0.159308]\n",
      "380it [00:09, 41.75it/s, Epoch=177, Loss=0.067179, Total Loss=0.079478]\n",
      "95it [00:01, 53.90it/s, Epoch=177, Val Loss=0.156589, Total Val Loss=0.157526]\n",
      "380it [00:09, 39.02it/s, Epoch=178, Loss=0.149157, Total Loss=0.080022]\n",
      "95it [00:01, 51.26it/s, Epoch=178, Val Loss=0.167650, Total Val Loss=0.163619]\n",
      "380it [00:09, 39.28it/s, Epoch=179, Loss=0.094768, Total Loss=0.079597]\n",
      "95it [00:01, 49.55it/s, Epoch=179, Val Loss=0.152816, Total Val Loss=0.157896]\n",
      "380it [00:09, 41.28it/s, Epoch=180, Loss=0.105246, Total Loss=0.078630]\n",
      "95it [00:01, 53.29it/s, Epoch=180, Val Loss=0.174959, Total Val Loss=0.158048]\n",
      "380it [00:08, 43.36it/s, Epoch=181, Loss=0.129747, Total Loss=0.079294]\n",
      "95it [00:01, 52.84it/s, Epoch=181, Val Loss=0.156769, Total Val Loss=0.157644]\n",
      "380it [00:10, 36.88it/s, Epoch=182, Loss=0.037200, Total Loss=0.078750]\n",
      "95it [00:01, 52.62it/s, Epoch=182, Val Loss=0.122031, Total Val Loss=0.158758]\n",
      "380it [00:09, 38.85it/s, Epoch=183, Loss=0.091853, Total Loss=0.078223]\n",
      "95it [00:01, 51.26it/s, Epoch=183, Val Loss=0.140154, Total Val Loss=0.159112]\n",
      "380it [00:09, 41.71it/s, Epoch=184, Loss=0.123788, Total Loss=0.078912]\n",
      "95it [00:01, 50.44it/s, Epoch=184, Val Loss=0.175070, Total Val Loss=0.161309]\n",
      "380it [00:09, 39.06it/s, Epoch=185, Loss=0.092237, Total Loss=0.076806]\n",
      "95it [00:01, 54.43it/s, Epoch=185, Val Loss=0.155692, Total Val Loss=0.160130]\n",
      "380it [00:09, 41.24it/s, Epoch=186, Loss=0.042000, Total Loss=0.076430]\n",
      "95it [00:01, 53.42it/s, Epoch=186, Val Loss=0.116974, Total Val Loss=0.158457]\n",
      "380it [00:08, 43.36it/s, Epoch=187, Loss=0.048938, Total Loss=0.076007]\n",
      "95it [00:01, 60.48it/s, Epoch=187, Val Loss=0.134065, Total Val Loss=0.157738]\n",
      "380it [00:08, 43.42it/s, Epoch=188, Loss=0.104702, Total Loss=0.077813]\n",
      "95it [00:01, 52.24it/s, Epoch=188, Val Loss=0.180319, Total Val Loss=0.158067]\n",
      "380it [00:08, 43.43it/s, Epoch=189, Loss=0.071350, Total Loss=0.076431]\n",
      "95it [00:01, 64.08it/s, Epoch=189, Val Loss=0.165020, Total Val Loss=0.160785]\n",
      "380it [00:08, 44.56it/s, Epoch=190, Loss=0.053386, Total Loss=0.075263]\n",
      "95it [00:01, 62.63it/s, Epoch=190, Val Loss=0.157056, Total Val Loss=0.159468]\n",
      "380it [00:08, 45.17it/s, Epoch=191, Loss=0.147571, Total Loss=0.075444]\n",
      "95it [00:01, 63.13it/s, Epoch=191, Val Loss=0.140350, Total Val Loss=0.158949]\n",
      "380it [00:08, 43.27it/s, Epoch=192, Loss=0.076286, Total Loss=0.075302]\n",
      "95it [00:01, 59.14it/s, Epoch=192, Val Loss=0.195901, Total Val Loss=0.159090]\n",
      "380it [00:09, 41.10it/s, Epoch=193, Loss=0.052856, Total Loss=0.075207]\n",
      "95it [00:01, 51.30it/s, Epoch=193, Val Loss=0.149907, Total Val Loss=0.159505]\n",
      "380it [00:09, 41.25it/s, Epoch=194, Loss=0.061428, Total Loss=0.075157]\n",
      "95it [00:01, 57.00it/s, Epoch=194, Val Loss=0.125530, Total Val Loss=0.160697]\n",
      "380it [00:09, 39.74it/s, Epoch=195, Loss=0.085755, Total Loss=0.075555]\n",
      "95it [00:02, 47.42it/s, Epoch=195, Val Loss=0.175960, Total Val Loss=0.159861]\n",
      "380it [00:09, 41.88it/s, Epoch=196, Loss=0.079152, Total Loss=0.075121]\n",
      "95it [00:01, 47.82it/s, Epoch=196, Val Loss=0.176983, Total Val Loss=0.157407]\n",
      "380it [00:09, 40.15it/s, Epoch=197, Loss=0.076716, Total Loss=0.076598]\n",
      "95it [00:01, 60.09it/s, Epoch=197, Val Loss=0.157634, Total Val Loss=0.161281]\n",
      "380it [00:09, 41.13it/s, Epoch=198, Loss=0.050804, Total Loss=0.075381]\n",
      "95it [00:01, 61.35it/s, Epoch=198, Val Loss=0.158514, Total Val Loss=0.160030]\n",
      "380it [00:09, 38.04it/s, Epoch=199, Loss=0.045409, Total Loss=0.074601]\n",
      "95it [00:01, 56.12it/s, Epoch=199, Val Loss=0.140670, Total Val Loss=0.158634]\n",
      "380it [00:08, 44.40it/s, Epoch=200, Loss=0.074649, Total Loss=0.072599]\n",
      "95it [00:01, 53.54it/s, Epoch=200, Val Loss=0.123629, Total Val Loss=0.159514]\n"
     ]
    }
   ],
   "source": [
    "loss_plot, val_loss_plot = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss, total_val_loss = 0, 0\n",
    "    \n",
    "    tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
    "    training = True\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_item.to(device)\n",
    "        batch_loss = train_step(batch_item, epoch, batch, training)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Loss': '{:06f}'.format(batch_loss.item()),\n",
    "            'Total Loss' : '{:06f}'.format(total_loss/(batch+1))\n",
    "        })\n",
    "    loss_plot.append(total_loss/(batch+1))\n",
    "    tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
    "    training = False\n",
    "    for batch, batch_item in tqdm_dataset:\n",
    "        batch_item.to(device)\n",
    "        batch_loss = train_step(batch_item, epoch, batch, training)\n",
    "        total_val_loss += batch_loss\n",
    "        \n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Val Loss': '{:06f}'.format(batch_loss.item()),\n",
    "            'Total Val Loss' : '{:06f}'.format(total_val_loss/(batch+1))\n",
    "        })\n",
    "    val_loss_plot.append(total_val_loss/(batch+1))\n",
    "    if np.min(val_loss_plot) == val_loss_plot[-1]:\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43995889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_plot, label='train_loss')\n",
    "plt.plot(val_loss_plot, label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss(mae)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f262b",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "923acf77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = GraphSAGE(dim_features, dim_target, config)\n",
    "loaded_model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8130ec46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0938, -0.0155, -0.0675,  ..., -0.0464, -0.0756, -0.0692],\n",
      "        [ 0.0307, -0.0758, -0.0135,  ..., -0.0044,  0.0205, -0.0124],\n",
      "        [-0.0746,  0.0075, -0.0420,  ...,  0.0946,  0.0084, -0.0159],\n",
      "        ...,\n",
      "        [-0.0405, -0.0818,  0.0133,  ..., -0.0894, -0.0912,  0.0730],\n",
      "        [-0.0903, -0.0517,  0.1414,  ...,  0.0946, -0.0051,  0.0728],\n",
      "        [-0.0846,  0.0626,  0.0271,  ..., -0.0529, -0.0608,  0.0351]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in loaded_model.parameters():\n",
    "    print(param)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59056405",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39dc1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv')\n",
    "submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d2691c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>SMILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>COc1ccc(S(=O)(=O)NC2CCN(C3CCCCC3)CC2)c(C)c1C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>CC(CCCC1CCC2C3=C(CC[C@]12C)[C@@]1(C)CC[C@H](C)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>C[C@@H]1C[C@@H]1c1ccc2c(c1)c(-c1ccc[nH]c1=O)c(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>CCCn1c(=O)c2ccccc2n2c(SCC(=O)NC(Cc3ccccc3)c3cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>CC(C)CN(C[C@@H](O)[C@H](Cc1ccccc1)NC(=O)OCc1cn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>test_597</td>\n",
       "      <td>N#Cc1c(-n2c3ccccc3c3ccccc32)c(-n2c3ccccc3c3ccc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>test_598</td>\n",
       "      <td>CC1(C)c2ccccc2N(c2ccc(-c3cc(-c4ccc(N5c6ccccc6C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>test_599</td>\n",
       "      <td>Cc1nc(-c2ccc(N3c4ccccc4C(C)(C)c4ccccc43)cc2)cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>test_600</td>\n",
       "      <td>c1ccc2c(c1)Oc1ccccc1N2c1ccc(-c2nc3ccccc3s2)cc1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>test_601</td>\n",
       "      <td>c1ccc2c(c1)Oc1ccccc1N2c1ccc(-c2nc3cc4sc(-c5ccc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>602 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          uid                                             SMILES\n",
       "0      test_0       COc1ccc(S(=O)(=O)NC2CCN(C3CCCCC3)CC2)c(C)c1C\n",
       "1      test_1  CC(CCCC1CCC2C3=C(CC[C@]12C)[C@@]1(C)CC[C@H](C)...\n",
       "2      test_2  C[C@@H]1C[C@@H]1c1ccc2c(c1)c(-c1ccc[nH]c1=O)c(...\n",
       "3      test_3  CCCn1c(=O)c2ccccc2n2c(SCC(=O)NC(Cc3ccccc3)c3cc...\n",
       "4      test_4  CC(C)CN(C[C@@H](O)[C@H](Cc1ccccc1)NC(=O)OCc1cn...\n",
       "..        ...                                                ...\n",
       "597  test_597  N#Cc1c(-n2c3ccccc3c3ccccc32)c(-n2c3ccccc3c3ccc...\n",
       "598  test_598  CC1(C)c2ccccc2N(c2ccc(-c3cc(-c4ccc(N5c6ccccc6C...\n",
       "599  test_599  Cc1nc(-c2ccc(N3c4ccccc4C(C)(C)c4ccccc43)cc2)cc...\n",
       "600  test_600     c1ccc2c(c1)Oc1ccccc1N2c1ccc(-c2nc3ccccc3s2)cc1\n",
       "601  test_601  c1ccc2c(c1)Oc1ccccc1N2c1ccc(-c2nc3cc4sc(-c5ccc...\n",
       "\n",
       "[602 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4669d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MoleculeDataset(root=\"../data/\", filename=\"test.csv\", test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0ebac45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch_geometric.data.dataloader.DataLoader at 0x7f044c241128>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=0, shuffle=False)\n",
    "test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "982aa91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset):\n",
    "    loaded_model.to(device)\n",
    "    loaded_model.eval()\n",
    "    result = []\n",
    "    for batch_item in dataset:\n",
    "        batch_item.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = loaded_model(batch_item)\n",
    "        output = output[:, 0]\n",
    "        output = output.cpu().numpy()\n",
    "        result.extend(output)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2da52803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>ST1_GAP(eV)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>0.799336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>1.827644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>1.284345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0.531842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>0.758733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>test_597</td>\n",
       "      <td>0.090313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>test_598</td>\n",
       "      <td>0.136329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>test_599</td>\n",
       "      <td>0.216002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>test_600</td>\n",
       "      <td>0.379241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>test_601</td>\n",
       "      <td>0.325335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>602 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          uid  ST1_GAP(eV)\n",
       "0      test_0     0.799336\n",
       "1      test_1     1.827644\n",
       "2      test_2     1.284345\n",
       "3      test_3     0.531842\n",
       "4      test_4     0.758733\n",
       "..        ...          ...\n",
       "597  test_597     0.090313\n",
       "598  test_598     0.136329\n",
       "599  test_599     0.216002\n",
       "600  test_600     0.379241\n",
       "601  test_601     0.325335\n",
       "\n",
       "[602 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = predict(test_dataloader)\n",
    "submission['ST1_GAP(eV)'] = pred\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "082cbb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 602 entries, 0 to 601\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   uid          602 non-null    object \n",
      " 1   ST1_GAP(eV)  602 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 9.5+ KB\n"
     ]
    }
   ],
   "source": [
    "submission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab78955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('dacon_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dacon_submit_api import dacon_submit_api \n",
    "\n",
    "result = dacon_submit_api.post_submission_file(\n",
    "    'dacon_baseline.csv', \n",
    "    '59375e1f3b2f6e39215d683eaee1a165fb30ef348d1da88be153f64c17dfe759', \n",
    "    '235789', \n",
    "    'melona', \n",
    "    'sage_2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e656ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
